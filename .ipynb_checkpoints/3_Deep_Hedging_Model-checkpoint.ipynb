{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d5a508-c46d-4e57-899a-bbdf36efc517",
   "metadata": {},
   "source": [
    "# 3. Deep Hedging Model\n",
    "\n",
    "In this notebook, we implement and train a neural network that learns to hedge a European call option dynamically using simulated market data.\n",
    "\n",
    "We treat this as a reinforcement learning-style problem, where the model (policy) observes the market, makes hedging decisions at each time step, and aims to replicate the option payoff as closely as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Model Architecture — GRU-Based Hedging Network\n",
    "\n",
    "We define a simple but powerful hedging policy based on a **Gated Recurrent Unit (GRU)**. This allows the model to retain memory of past states and learn sequential dependencies, which are important for financial time series.\n",
    "\n",
    "- **Input**:  \n",
    "  - A state vector $( s_t \\in \\mathbb{R}^4 )$ at each time step, including:\n",
    "    1. **Normalized Price**: $( S_t / S_0 )$\n",
    "    2. **Time to Maturity**: $( (T - t)/T )$\n",
    "    3. **Log Return**: $( \\log(S_t / S_{t-1}) )$\n",
    "    4. **Simulated Volatility**: $( \\sigma_t )$\n",
    "\n",
    "- **Output**:  \n",
    "  - A hedge ratio $( \\Delta_t \\in \\mathbb{R} )$ for each time step\n",
    "- **Network**:\n",
    "  - GRU layer with 64 hidden units\n",
    "  - Fully connected layer to predict hedge ratio at each step\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Training Setup\n",
    "\n",
    "We use a custom loss function inspired by financial risk management:\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "We combine:\n",
    "- **ITM-CVaR**: Conditional Value at Risk over in-the-money paths  \n",
    "- **MSE penalty**: Penalizes both undershooting and overshooting the target payoff\n",
    "\n",
    "The full loss function is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\text{CVaR}_\\alpha(V_T - Z_T) + \\beta \\cdot \\text{MSE}_{\\text{ITM}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $( V_T )$: terminal value of the hedged portfolio\n",
    "- $( Z_T )$: true payoff of the option\n",
    "- $( \\alpha )$: CVaR level (e.g., 0.9)\n",
    "- $( \\beta )$: small weight (e.g., 0.1) to control MSE influence\n",
    "\n",
    "We use:\n",
    "- Optimizer: Adam\n",
    "- Learning rate scheduler: ReduceLROnPlateau\n",
    "- Training for 300 epochs on 8,000 simulated paths\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Trading Logic\n",
    "\n",
    "At each step:\n",
    "- The model adjusts its hedge ratio $( \\Delta_t )$\n",
    "- We simulate the P&L of adjusting the position, considering:\n",
    "  - Change in position\n",
    "  - Asset price\n",
    "  - Transaction costs (set to 0.0 here, but can be tuned)\n",
    "- At maturity $( T )$, the final portfolio value is:\n",
    "\n",
    "$$\n",
    "V_T = \\text{cash} + \\Delta_T \\cdot S_T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Logging and Evaluation\n",
    "\n",
    "- Loss and learning rate are logged every 10 epochs\n",
    "- Evaluation is performed on 2,000 out-of-sample test paths\n",
    "- Results (portfolio values and payoffs) are saved to:\n",
    "  - `results/hedging_eval_test.npz`\n",
    "- Model weights are saved to:\n",
    "  - `models/gru_beta010_final.pt`\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Summary\n",
    "\n",
    "This notebook implemented:\n",
    "- A sequential GRU-based hedging model\n",
    "- A custom objective combining CVaR and MSE\n",
    "- Full training loop with adaptive learning rate\n",
    "- Evaluation and saving of the final results\n",
    "\n",
    "This completes the learning phase. In the next notebook, we will **backtest** and **compare performance** against benchmarks like Delta Hedging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff06fac-ac7d-4287-b1bc-d308954f9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, torch, platform, os, site, json, textwrap, subprocess, pathlib, gc\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import trange\n",
    "# import time\n",
    "# from IPython.display import clear_output\n",
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093382c5-8f8a-40a1-9e15-af8a2a776ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"data/deep_hedging_data.npz\")\n",
    "X_train = data[\"X_train\"]            # (8000, T, 4)\n",
    "Y_train = data[\"Y_train\"]            # (8000,)\n",
    "X_test  = data[\"X_test\"]             # (2000, T, 4)\n",
    "Y_test  = data[\"Y_test\"]             # (2000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee57b39-bcce-48c0-b7fd-97b6bff44c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[████████████████████████████████████████] 100.0%  (300/300)\n",
      "\n",
      "Saved results to results/hedging_eval_test.npz  (shape: torch.Size([2000]))  in 0.2s\n"
     ]
    }
   ],
   "source": [
    "import torch, math, time, os\n",
    "from torch import nn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0.  Load tensors (already have X_train, Y_train, X_test, Y_test in RAM)\n",
    "# ------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32, device=device).squeeze(-1)\n",
    "X_test  = torch.tensor(X_test , dtype=torch.float32, device=device)\n",
    "Y_test  = torch.tensor(Y_test , dtype=torch.float32, device=device).squeeze(-1)\n",
    "\n",
    "N_train, T, _ = X_train.shape        # 8000 × T × 4\n",
    "N_test  = X_test.shape[0]\n",
    "\n",
    "# quick imbalance print\n",
    "zero_pct = (Y_train == 0).float().mean().item() * 100\n",
    "print(f\"{zero_pct:.1f}% of training pay-offs are exactly zero\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build spot matrices from feature-0 (normalised price)\n",
    "# ------------------------------------------------------------\n",
    "S0   = 100.0                         # same S0 you used in Notebook 2\n",
    "S_t  = X_train[:, :, 0] * S0         # [8000, T]  — already on GPU\n",
    "S_te = X_test  [:, :, 0] * S0        # [2000, T]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  GRU hedge network\n",
    "# ------------------------------------------------------------\n",
    "class HedgingGRU(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):            # x: [N, T, 4]\n",
    "        h, _ = self.gru(x)\n",
    "        return self.out(h).squeeze(-1)   # [N, T]\n",
    "\n",
    "model = HedgingGRU().to(device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Optimiser & scheduler\n",
    "# ------------------------------------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "              optimizer, factor=0.5, patience=20, verbose=False)\n",
    "\n",
    "n_epochs = 300\n",
    "tc       = 0.0\n",
    "alpha    = 0.9\n",
    "logfile  = open(\"training_log.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "def progress_bar(i, total, bar_len=40):\n",
    "    pct  = i / total\n",
    "    fill = \"█\" * int(bar_len * pct)\n",
    "    bar  = f\"[{fill:<{bar_len}}] {pct:6.1%}  ({i}/{total})\"\n",
    "    clear_output(wait=True); print(bar)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Training loop\n",
    "# ------------------------------------------------------------\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # hedge ratios\n",
    "    H   = model(X_train)                                # [N, T]\n",
    "    dH  = torch.diff(H, 1, prepend=torch.zeros_like(H[:, :1]))\n",
    "\n",
    "    trade_vol = dH.abs() * S_t\n",
    "    cash      = (-dH * S_t - tc * trade_vol).sum(1)\n",
    "    V_T       = cash + H[:, -1] * S_t[:, -1]            # [N]\n",
    "\n",
    "        # --- ITM-CVaR(α)  +  β·MSE (to penalise overshoot) ----------------\n",
    "    itm   = Y_train > 0\n",
    "    itm_err = (V_T - Y_train)[itm]        # P&L error on ITM paths\n",
    "\n",
    "    beta = 0.10                           # 0.01–0.10 possible values\n",
    "\n",
    "    if itm_err.numel():\n",
    "        # 1) CVaR part (same as before)\n",
    "        short = torch.clamp(itm_err, max=0)               # only short-falls\n",
    "        k     = max(int((1 - alpha) * itm_err.numel()), 1)\n",
    "        VaR, _ = torch.kthvalue(short, k)\n",
    "        cvar   = short[short <= VaR].mean()               # ≤ 0\n",
    "        # 2) symmetric MSE part (penalise overshoot too)\n",
    "        mse_itm = (itm_err**2).mean()\n",
    "        # 3) combined objective\n",
    "        loss = -cvar + beta * mse_itm                     # minimise both\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        msg = (f\"epoch {epoch:4d}  -CVaR {(-loss).item():10.4f}  \"\n",
    "               f\"lr {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "        logfile.write(msg + \"\\n\"); logfile.flush()\n",
    "        print(msg)\n",
    "    progress_bar(epoch + 1, n_epochs)\n",
    "\n",
    "logfile.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  TEST EVALUATION & SAVE\n",
    "# ------------------------------------------------------------\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    H_te   = model(X_test)                               # [2000, T]\n",
    "    dH_te  = torch.diff(H_te, 1, prepend=torch.zeros_like(H_te[:, :1]))\n",
    "    trade_vol = dH_te.abs() * S_te\n",
    "    cash   = (-dH_te * S_te - tc * trade_vol).sum(1)\n",
    "    V_T_te = cash + H_te[:, -1] * S_te[:, -1]            # [2000]\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    \"results/hedging_eval_test.npz\",\n",
    "    V_T = V_T_te.cpu().numpy().astype(np.float32),\n",
    "    Z_T = Y_test.cpu().numpy().astype(np.float32)\n",
    ")\n",
    "print(f\"\\nSaved results to results/hedging_eval_test.npz  \"\n",
    "      f\"(shape: {V_T_te.shape})  in {time.time()-start:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784cecb0-bfdf-4ea0-bc7a-0da781c35ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓  Saved weights to  models/gru_beta010_final.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/gru_beta010_final.pt\")\n",
    "print(\"✓  Saved weights to  models/gru_beta010_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clustering-env)",
   "language": "python",
   "name": "clustering-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
