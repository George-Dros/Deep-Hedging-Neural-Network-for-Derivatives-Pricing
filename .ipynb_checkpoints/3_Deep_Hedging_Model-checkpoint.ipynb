{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d5a508-c46d-4e57-899a-bbdf36efc517",
   "metadata": {},
   "source": [
    "# 3. Deep Hedging Model\n",
    "\n",
    "In this notebook, we implement and train a neural network that learns to hedge a European call option dynamically using simulated market data.\n",
    "\n",
    "> **Goal.** Train a trading policy $(a_t)$ that minimizes **tail risk** (CVaR) of the terminal hedging error\n",
    "> $$\n",
    "X \\;=\\; -Z_T \\;+\\; \\sum_{t=0}^{T-1} a_t\\,\\Delta S_t \\;-\\; \\underbrace{\\gamma\\sum_{t=0}^{T-1} S_t\\,|a_t - a_{t-1}|}_{\\text{transaction cost}}\n",
    " $$\n",
    "> on simulated paths from Notebook 2. We do this end-to-end with a recurrent policy network, a symbolic rollout, and a Rockafellar–Uryasev (RU) CVaR head.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1) What this notebook consumes / produces\n",
    "\n",
    "**Inputs (from Notebook 2):**\n",
    "- `data/S_train.npy`, `S_val.npy`, `S_test.npy`  — each shape `(N, n_steps+1)`\n",
    "- `data/Z_T_train.npy`, `Z_T_val.npy`, `Z_T_test.npy`  — each shape `(N,)`\n",
    "- `data/meta.json` — `S0`, `K`, `T`, `n_steps`, `dt`, plus knobs like `alpha`, `gamma`, `h_max` (optional)\n",
    "\n",
    "**Outputs (used by Notebook 4):**\n",
    "- Trained weights (best by tail metrics)\n",
    "- Arrays for backtesting: $(X)$, $(\\text{PnL})$, $(\\text{cost})$, $(\\text{turnover})$, $(Z_T)$, and optionally positions $(a_t)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2) Setup & reproducibility\n",
    "\n",
    "**What to do here:**\n",
    "- Import `numpy`, `tensorflow` (Keras), `json`, `pathlib`.\n",
    "- Fix seeds (`tf.keras.utils.set_random_seed(SEED)`).\n",
    "- Read `meta.json`, unpack: `S0, K, T_years, n_steps, dt`.  \n",
    "- Choose training knobs (typical defaults shown; adjust later in §12):\n",
    "  - `ALPHA = 0.90` (CVaR level)\n",
    "  - `GAMMA = 0.001` (turnover cost scale; ≈10 bps)\n",
    "  - `H_MAX = 5` (position clamp \\(|a_t|\\le H_{\\max}\\))\n",
    "  - `BATCH`, `EPOCHS`, `LR`\n",
    "- Load arrays `S_*` and `Z_T_*` and print shapes (sanity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee57b39-bcce-48c0-b7fd-97b6bff44c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: S_train (8000, 253), S_val (5000, 253), S_test (5000, 253)\n",
      "Payoffs: Z_T_train (8000,), Z_T_val (5000,), Z_T_test (5000,)\n",
      "Strike Prices: K_train (8000,), K_val (5000,), K_test (5000,)\n",
      "n_steps = 252, N_train = (8000,), N_val = (5000,), N_test = (5000,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 3 (Keras) — Deep Hedging with CVaR Utility\n",
    "# ============================================================\n",
    "# Requirements: TensorFlow 2.x (tf.keras), numpy, Keras 3\n",
    "# Inputs:   data/S_train.npy, S_val.npy, S_test.npy\n",
    "#           data/Z_T_train.npy, Z_T_val.npy, Z_T_test.npy\n",
    "#           data/meta.json  (S0, K, T, n_steps, dt, alpha, etc.)\n",
    "# Output:   results/hedging_eval_test_keras.npz  (keys: V_T, Z_T)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, math, time\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # quiet TF logs\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# --------------------------\n",
    "# 0) Repro + folders\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "RESULTS_DIR = Path(\"results\"); RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# 1) Load meta + raw arrays\n",
    "# --------------------------\n",
    "with open(DATA_DIR/\"meta_x.json\", \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "S0       = float(meta[\"S0\"])\n",
    "K        = float(meta[\"K\"])\n",
    "T_years  = float(meta[\"T\"])\n",
    "n_steps  = int(meta[\"n_steps\"])\n",
    "dt       = float(meta[\"dt\"])\n",
    "\n",
    "# --------------------------\n",
    "# 2) Training knobs\n",
    "# --------------------------\n",
    "ALPHA  = 0.9 # CVaR tail level\n",
    "GAMMA  = 0.0005 # Transaction cost scale γ = 0.001 (10 bps per unit turnover)\n",
    "H_MAX  = 3 # max abs(position)\n",
    "BATCH  = 600 # for ALPHA=0.95 ~ 400\n",
    "EPOCHS = 150\n",
    "LR     = 5e-5\n",
    "\n",
    "opt_train = np.load(DATA_DIR/\"opt_train_vol.npy\")\n",
    "opt_val   = np.load(DATA_DIR/\"opt_val_vol.npy\")\n",
    "opt_test  = np.load(DATA_DIR/\"opt_test_vol.npy\")\n",
    "\n",
    "S_train = np.load(DATA_DIR/\"S_train_vol.npy\")   # (N_train, n_steps+1)\n",
    "S_val   = np.load(DATA_DIR/\"S_val_vol.npy\")\n",
    "S_test  = np.load(DATA_DIR/\"S_test_vol.npy\")\n",
    "\n",
    "K_train = np.load(DATA_DIR/\"K_train_vol.npy\")   # (N_train,)\n",
    "K_val   = np.load(DATA_DIR/\"K_val_vol.npy\")\n",
    "K_test  = np.load(DATA_DIR/\"K_test_vol.npy\")\n",
    "\n",
    "Z_T_train = np.load(DATA_DIR/\"Z_T_train_vol.npy\").astype(np.float32)\n",
    "Z_T_val   = np.load(DATA_DIR/\"Z_T_val_vol.npy\").astype(np.float32)\n",
    "Z_T_test  = np.load(DATA_DIR/\"Z_T_test_vol.npy\").astype(np.float32)\n",
    "\n",
    "print(f\"Loaded: S_train {S_train.shape}, S_val {S_val.shape}, S_test {S_test.shape}\")\n",
    "print(f\"Payoffs: Z_T_train {Z_T_train.shape}, Z_T_val {Z_T_val.shape}, Z_T_test {Z_T_test.shape}\")\n",
    "print(f\"Strike Prices: K_train {K_train.shape}, K_val {K_val.shape}, K_test {K_test.shape}\")\n",
    "print(f\"n_steps = {n_steps}, N_train = {S_train[:,0].shape}, N_val = {S_val[:,0].shape}, N_test = {S_test[:,0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7d406-2cec-4d46-ab56-c85db1cead5a",
   "metadata": {},
   "source": [
    "## 3.3) Build *causal* features and $(\\Delta S)$\n",
    "\n",
    "**What to do here:**\n",
    "- From each `S` (shape `(N, n_steps+1)`) compute:\n",
    "  - $(S_t := S[:, 0:n\\_steps])$\n",
    "  - $(S_{t+1} := S[:, 1:n\\_steps+1])$\n",
    "  - $(\\Delta S_t = S_{t+1} - S_t)$ → shape `(N, n_steps, 1)`\n",
    "- Causal features per time step $(t)$ (shape `(N, n_steps, 6)`):\n",
    "  1. `price_norm`: $(S_t / S_0)$\n",
    "  2. `moneyness`: $(K_t / S_0)$\n",
    "  3. `tau`: linear grid $(\\tau_t = (T - t)/T)$\n",
    "  4. `logR_past`: **lagged** log return; at $(t=0)$ set to `0`, for $(t>0)$ use $(\\log S_t - \\log S_{t-1})$\n",
    "  5. `vol_ann`: rolling std of `logR_past` over a window (e.g., 20), annualized by $(\\sqrt{1/dt})$\n",
    "  6. `normalized Delta`: $\\Delta$\n",
    "  7. `normalized Gamma`: $\\Gamma$\n",
    "  8. `normalized Vega`: $Vega$\n",
    "  9. `option_type`: Call/Put, binary values 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8482d5e0-69fd-4644-a711-785850a81735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature tensor shapes: \n",
      " dSt_train: (8000, 252, 1) \n",
      "  X_train: (8000, 252, 9) \n",
      "  X_val:   (5000, 252, 9) \n",
      "  X_test:  (5000, 252, 9) \n",
      "  K_train: (8000,) \n",
      "  K_val:   (5000,) \n",
      "  K_test:  (5000,) \n",
      "  Z_train: (8000,) \n",
      "  Z_val:   (5000,) \n",
      "  Z_test:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3) Causal features: [price_norm, moneyness, tau, log_return, realized_vol_annualized, norm_delta, norm_gamma, norm_vega, call/put]\n",
    "# --------------------------\n",
    "def bs_delta(St, K, tau, vol_ann, opt_type, r=0.0):\n",
    "    \"\"\"\n",
    "    St: (N, n_steps)\n",
    "    K:  (N,) (strike per path)\n",
    "    tau, vol_ann: (N, n_steps)\n",
    "    opt_type: (N,)  0=call, 1=put\n",
    "    returns delta: (N, n_steps)\n",
    "    \"\"\"\n",
    "    K = np.asarray(K).reshape(-1)\n",
    "    Kt = K[:, None]   # broadcast\n",
    "    \n",
    "    eps = 1e-6\n",
    "    tau_safe = np.clip(tau, eps, None)\n",
    "    vol_safe = np.clip(vol_ann, eps, None)\n",
    "\n",
    "    # avoid divide-by-zero where tau == 0\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        d1 = np.where(\n",
    "            tau_safe > 0,\n",
    "            (np.log(St / Kt) + (r + 0.5 * vol_safe**2) * tau_safe) / (vol_safe * np.sqrt(tau_safe)),\n",
    "            0.0\n",
    "        )\n",
    "\n",
    "    delta = np.where(opt_type[:, None] == 0, norm.cdf(d1), norm.cdf(d1) - 1.0)\n",
    "    return delta\n",
    "\n",
    "def bs_d1_phi(St, K, tau, vol_ann, r=0.0, eps=1e-6):\n",
    "    \"\"\"\n",
    "    St: (N, n_steps)   - spot per path/time (not including final step)\n",
    "    K:  (N,)           - strike per path (constant)\n",
    "    tau, vol_ann: (N, n_steps)\n",
    "    returns: d1, phi, vol_safe, tau_safe  (all shape (N, n_steps))\n",
    "    \"\"\"\n",
    "    K = np.asarray(K).reshape(-1)            # ensure 1d\n",
    "    Kt = K[:, None]                          # broadcast to (N, n_steps)\n",
    "\n",
    "    tau_safe = np.clip(tau, eps, None)\n",
    "    vol_safe = np.clip(vol_ann, eps, None)\n",
    "\n",
    "    d1 = (np.log(St / Kt) + (r + 0.5 * vol_safe**2) * tau_safe) / (vol_safe * np.sqrt(tau_safe))\n",
    "    d1 = np.clip(d1, -10.0, 10.0)\n",
    "    phi = norm.pdf(d1)\n",
    "    return d1, phi, vol_safe, tau_safe\n",
    "\n",
    "\n",
    "# ---------- Features builder (robust to K being array or matrix) ----------\n",
    "def build_features_from_S(S, K, S0, n_steps, dt, win=20):\n",
    "    \"\"\"\n",
    "    S: (N, n_steps+1)\n",
    "    K: (N,) or (N, n_steps+1). This function handles either but will\n",
    "       prefer treating K as per-path strike if 1D.\n",
    "    Returns: dSt, dK, X, logR_past, vol_ann\n",
    "    \"\"\"\n",
    "    S = np.asarray(S)\n",
    "    K = np.asarray(K)\n",
    "\n",
    "    N = S.shape[0]\n",
    "    St = S[:, :-1]        # (N, n_steps)\n",
    "    St_p1 = S[:, 1:]\n",
    "\n",
    "    # dS shape (N, n_steps, 1)\n",
    "    dSt = (St_p1 - St)[:, :, None].astype(np.float32)\n",
    "\n",
    "    price_norm = St / S0\n",
    "\n",
    "    # Standardize strike shape:\n",
    "    if K.ndim == 1:\n",
    "        # strike per path (constant through time)\n",
    "        Kt = np.broadcast_to(K[:, None], St.shape)  # (N, n_steps)\n",
    "        # dK is zero if strike is constant\n",
    "        dK = np.zeros_like(dSt, dtype=np.float32)\n",
    "    elif K.ndim == 2:\n",
    "        # strike matrix: use all but last column to align with St\n",
    "        if K.shape[1] != St.shape[1] + 1:\n",
    "            raise ValueError(f\"K matrix must have n_steps+1 columns to align with S. got {K.shape}\")\n",
    "        Kt = K[:, :-1]\n",
    "        dK = (K[:, 1:] - K[:, :-1])[:, :, None].astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected K ndim: {K.ndim}\")\n",
    "\n",
    "    # Avoid zero division early:\n",
    "    Kt = Kt.astype(np.float32)\n",
    "\n",
    "    moneyness = (St - Kt) / (Kt + 1e-12)\n",
    "\n",
    "    # log returns (for vol feature)\n",
    "    r_next = np.diff(np.log(S), axis=1).astype(np.float32)\n",
    "    logR_past = np.zeros_like(r_next, dtype=np.float32)\n",
    "    if n_steps > 1:\n",
    "        logR_past[:, 1:] = r_next[:, :-1]\n",
    "\n",
    "    # tau vector (normalized time remaining)\n",
    "    tau_vec = np.linspace(1.0 - 1.0/n_steps, 0.0, n_steps, dtype=np.float32)\n",
    "    tau = np.broadcast_to(tau_vec, (N, n_steps)).astype(np.float32)\n",
    "\n",
    "    # realized vol (rolling past window)\n",
    "    vol = np.empty_like(logR_past, dtype=np.float32)\n",
    "    for t in range(n_steps):\n",
    "        l = max(0, t - win + 1)\n",
    "        w = logR_past[:, l:t+1]\n",
    "        vol[:, t] = w.std(axis=1)\n",
    "    steps_per_year = int(round(1.0 / dt))\n",
    "    vol_ann = vol * np.sqrt(steps_per_year)\n",
    "\n",
    "    # Greeks using broadcast-friendly functions\n",
    "    d1, phi, vol_safe, tau_safe = bs_d1_phi(St, K if K.ndim == 1 else Kt, tau, vol_ann, r=0.0, eps=1e-6)\n",
    "    gamma = phi / (St * vol_safe * np.sqrt(tau_safe) + 1e-12)\n",
    "    vega  = St * phi * np.sqrt(tau_safe)\n",
    "    gamma = np.clip(gamma, -1e3, 1e3)\n",
    "    vega  = np.clip(vega, -1e3, 1e3)\n",
    "\n",
    "    X = np.stack([price_norm, moneyness, tau, logR_past, vol_ann, gamma, vega], axis=-1).astype(np.float32)\n",
    "\n",
    "    return dSt, dK, X, logR_past.astype(np.float32), vol_ann.astype(np.float32)\n",
    "    \n",
    "dSt_train, dK_train, X_train, logR_train, vol_train = build_features_from_S(S_train, K_train, S0, n_steps, dt, win=20)\n",
    "dSt_val, dK_val, X_val,   logR_val,   vol_val   = build_features_from_S(S_val, K_val,   S0, n_steps, dt, win=20)\n",
    "dSt_test, dK_test, X_test,  logR_test,  vol_test  = build_features_from_S(S_test, K_test,  S0, n_steps, dt, win=20)\n",
    "\n",
    "Z_T_train = np.asarray(Z_T_train, dtype=np.float32).reshape(-1)\n",
    "Z_T_val   = np.asarray(Z_T_val,   dtype=np.float32).reshape(-1)\n",
    "Z_T_test  = np.asarray(Z_T_test,  dtype=np.float32).reshape(-1)\n",
    "\n",
    "# Use the raw S arrays (they exist in your notebook)\n",
    "St_train = S_train[:, :-1]   # shape (N, n_steps)\n",
    "St_val   = S_val[:, :-1]\n",
    "St_test  = S_test[:, :-1]\n",
    "\n",
    "delta_train = bs_delta(St_train, K_train, X_train[:, :, 2], X_train[:, :, 4], opt_train, r=0.0)\n",
    "delta_val   = bs_delta(St_val,   K_val,   X_val[:, :, 2],   X_val[:, :, 4],   opt_val,   r=0.0)\n",
    "delta_test  = bs_delta(St_test,  K_test,  X_test[:, :, 2],  X_test[:, :, 4],  opt_test,  r=0.0)\n",
    "\n",
    "\n",
    "# ---- Broadcast Delta along last dim to make it (N, n_steps, 1) like your other features ----\n",
    "delta_train_feat = delta_train[:,:,None].astype(np.float32)\n",
    "delta_val_feat   = delta_val[:,:,None].astype(np.float32)\n",
    "delta_test_feat  = delta_test[:,:,None].astype(np.float32)\n",
    "\n",
    "# ---- NEW option type ----\n",
    "opt_train = np.load(DATA_DIR/\"opt_train_x.npy\")  # shape (N_train,)\n",
    "opt_val   = np.load(DATA_DIR/\"opt_val_x.npy\")\n",
    "opt_test  = np.load(DATA_DIR/\"opt_test_x.npy\")\n",
    "\n",
    "# Broadcast each option type along the time axis so it’s (N, n_steps, 1)\n",
    "opt_train_feat = np.tile(opt_train[:, None, None], (1, X_train.shape[1], 1))\n",
    "opt_val_feat   = np.tile(opt_val[:, None, None],   (1, X_val.shape[1], 1))\n",
    "opt_test_feat  = np.tile(opt_test[:, None, None],  (1, X_test.shape[1], 1))\n",
    "\n",
    "# ---- Concatenate to existing features ----\n",
    "X_train = np.concatenate([X_train, delta_train_feat], axis=-1)\n",
    "X_val   = np.concatenate([X_val,   delta_val_feat],   axis=-1)\n",
    "X_test  = np.concatenate([X_test,  delta_test_feat],  axis=-1)\n",
    "\n",
    "# Extract Greeks correctly: (N, n_steps, 3)\n",
    "greeks_train = X_train[:, :, -3:]\n",
    "greeks_val   = X_val[:, :, -3:]\n",
    "greeks_test  = X_test[:, :, -3:]\n",
    "\n",
    "# Flatten to (N*n_steps, 3) for sklearn\n",
    "greeks_train_flat = greeks_train.reshape(-1, 3)\n",
    "greeks_val_flat   = greeks_val.reshape(-1, 3)\n",
    "greeks_test_flat  = greeks_test.reshape(-1, 3)\n",
    "\n",
    "# Normalize only greeks\n",
    "scaler_greeks = StandardScaler()\n",
    "greeks_train_norm = scaler_greeks.fit_transform(greeks_train_flat)\n",
    "greeks_val_norm   = scaler_greeks.transform(greeks_val_flat)\n",
    "greeks_test_norm  = scaler_greeks.transform(greeks_test_flat)\n",
    "\n",
    "# Reshape back to (N, n_steps, 3)\n",
    "X_train[:, :, -3:] = greeks_train_norm.reshape(X_train.shape[0], X_train.shape[1], 3)\n",
    "X_val[:, :, -3:]   = greeks_val_norm.reshape(X_val.shape[0], X_val.shape[1], 3)\n",
    "X_test[:, :, -3:]  = greeks_test_norm.reshape(X_test.shape[0], X_test.shape[1], 3)\n",
    "\n",
    "# Concatenate with the 4 existing features → now we have 5\n",
    "X_train = np.concatenate([X_train, opt_train_feat], axis=-1)\n",
    "X_val   = np.concatenate([X_val,   opt_val_feat],   axis=-1)\n",
    "X_test  = np.concatenate([X_test,  opt_test_feat],  axis=-1)\n",
    "\n",
    "\n",
    "print(\"Feature tensor shapes:\",\n",
    "      \"\\n dSt_train:\", dSt_train.shape,\n",
    "      \"\\n  X_train:\", X_train.shape,\n",
    "      \"\\n  X_val:  \", X_val.shape,\n",
    "      \"\\n  X_test: \", X_test.shape,\n",
    "      \"\\n  K_train:\", K_train.shape,\n",
    "      \"\\n  K_val:  \", K_val.shape,\n",
    "      \"\\n  K_test: \", K_test.shape,\n",
    "      \"\\n  Z_train:\", Z_T_train.shape,\n",
    "      \"\\n  Z_val:  \", Z_T_val.shape,\n",
    "      \"\\n  Z_test: \", Z_T_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86acdbe-d50c-4311-87ea-f734ef43df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat   = X_train.shape[-1] \n",
    "n_train  = X_train.shape[0]\n",
    "n_val  = X_val.shape[0]\n",
    "n_test  = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1ee6a-753d-442f-b590-547f44c0c59d",
   "metadata": {},
   "source": [
    "## 3.4) Build `tf.data` pipelines\n",
    "\n",
    "**What to do here:**\n",
    "- Construct dictionaries per split:\n",
    "  - `inputs = {\"features\": X, \"dS\": dS, \"Z_T\": Z_T}`  \n",
    "    (keep `Z_T` **1D** `(N,)`\n",
    "  - `labels = zeros(N)`  (dummy; our loss reads from the graph)\n",
    "- Create datasets:\n",
    "  - `train_ds = Dataset.from_tensor_slices((inputs, labels)).cache().shuffle(...).batch(BATCH).prefetch(AUTOTUNE)`\n",
    "  - Similarly for `val_ds` (no shuffle) and `test_ds`.\n",
    "- **Peek a batch** and print dtypes/shapes to confirm:\n",
    "  - `features: (BATCH, n_steps, 4)`\n",
    "  - `dS:       (BATCH, n_steps, 1)`\n",
    "  - `Z_T:      (BATCH,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902a04d1-a0f0-49e7-8753-ad87d8e4e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4) Datasets\n",
    "# --------------------------\n",
    "\n",
    "inputs = {\n",
    "    \"features\": X_train[:,:,:],\n",
    "    \"dS\": dSt_train[:,:,:],\n",
    "    \"Z_T\": Z_T_train[:]\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    \"features\": X_val[:,:,:],\n",
    "    \"dS\": dSt_val[:,:,:],\n",
    "    \"Z_T\": Z_T_val[:]\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    \"features\": X_test[:,:,:],\n",
    "    \"dS\": dSt_test[:,:,:],\n",
    "    \"Z_T\": Z_T_test[:]\n",
    "}\n",
    "\n",
    "input_label = np.zeros((n_train),dtype=np.float32)\n",
    "val_label = np.zeros((n_val),dtype=np.float32)\n",
    "test_label = np.zeros((n_test),dtype=np.float32)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((inputs, input_label))\n",
    "            .cache().shuffle(8192, seed=SEED, reshuffle_each_iteration=True)\n",
    "            .batch(BATCH).prefetch(AUTOTUNE))\n",
    "\n",
    "val_ds   = (tf.data.Dataset.from_tensor_slices((val_inputs, val_label))\n",
    "            .cache().batch(BATCH).prefetch(AUTOTUNE))\n",
    "\n",
    "test_ds  = (tf.data.Dataset.from_tensor_slices((test_inputs, test_label))\n",
    "            .batch(BATCH).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db49a91-1e5f-4efc-8579-2528221b8d6c",
   "metadata": {},
   "source": [
    "## 3.5) Zero-hedge baseline (validation)\n",
    "\n",
    "**What to do here:**\n",
    "- Baseline wealth if you **do not trade**: $(X^{(0)} = -Z_T)$.\n",
    "- Compute on **all** validation paths:\n",
    "  - `mean_baseline = mean(X0)`\n",
    "  - `VaR_α` and `CVaR_α` at `ALPHA` (note: sort ascending, take worst tail)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08fc1c1-da37-4ee7-9d4a-541b468a1911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_cvar_baseline: -105.294373\n",
      "val_mean_baseline: -22.909412\n",
      "val_p05_baseline: -88.195668\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5) Baseline \n",
    "# --------------------------\n",
    "wealth = - Z_T_val\n",
    "\n",
    "val_mean_baseline  = wealth.mean()\n",
    "sorted_wealth = np.sort(wealth)\n",
    "\n",
    "k = int(np.ceil((1 - ALPHA) * sorted_wealth.size))\n",
    "values = sorted_wealth[:k]\n",
    "\n",
    "val_cvar_baseline = values.mean()\n",
    "val_p05_baseline = np.percentile(wealth, 5)\n",
    "\n",
    "print(f\"val_cvar_baseline: {val_cvar_baseline:.6f}\")\n",
    "print(f\"val_mean_baseline: {val_mean_baseline:.6f}\")\n",
    "print(f\"val_p05_baseline: {val_p05_baseline:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541c9f4-8835-46f7-b17d-4ab00f516b9f",
   "metadata": {},
   "source": [
    "## 3.6) Policy network (backbone)\n",
    "\n",
    "**What to do here:**\n",
    "- Define three **Inputs**:\n",
    "  - `features`: `(n_steps, 4)` — the only tensor used by the policy\n",
    "  - `dS`: `(n_steps, 1)` — used *later* in rollout, not inside the policy\n",
    "  - `Z_T`: `()` — scalar terminal payoff, used *later* in rollout, not inside the policy\n",
    "- Build a small recurrent backbone, e.g. GRU→GRU→Dense:\n",
    "  - GRU(32, return_sequences=True) → GRU(16, return_sequences=True) → Dense(32, relu)\n",
    "  - Final Dense to 1 → raw position `a_raw(t)`\n",
    "  - **Clamp**: `a(t) = H_MAX * tanh(a_raw(t))` to bound $(|a_t|\\le H_{\\max})$\n",
    "\n",
    "**Why these choices:**\n",
    "- GRUs capture temporal context without heavy complexity.\n",
    "- `tanh` clamp stabilizes training, enforces realistic position bounds.\n",
    "- `dS` and `Z_T` must *not* leak into the policy — they are used only in P&L computation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00db88c7-8672-4048-87de-95e661664b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5691d57-47bc-4423-9fdc-7f1a7d2e48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"policy_backbone\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " features (InputLayer)       [(None, 252, 9)]             0         []                            \n",
      "                                                                                                  \n",
      " gru_32 (GRU)                (None, 252, 32)              4128      ['features[0][0]']            \n",
      "                                                                                                  \n",
      " gru_16 (GRU)                (None, 252, 16)              2400      ['gru_32[0][0]']              \n",
      "                                                                                                  \n",
      " dense_32 (Dense)            (None, 252, 32)              544       ['gru_16[0][0]']              \n",
      "                                                                                                  \n",
      " a_raw (Dense)               (None, 252, 1)               33        ['dense_32[0][0]']            \n",
      "                                                                                                  \n",
      " dS (InputLayer)             [(None, 252, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " Z_T (InputLayer)            [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " a (Lambda)                  (None, 252, 1)               0         ['a_raw[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7105 (27.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6) Model\n",
    "# --------------------------\n",
    "\n",
    "features_in = keras.Input(shape=(n_steps, n_feat), name=\"features\")\n",
    "dS_in = keras.Input(shape=(n_steps,1), name=\"dS\")\n",
    "Z_T_in = keras.Input(shape=(), name=\"Z_T\")\n",
    "\n",
    "H = layers.GRU(32, return_sequences=True, name=\"gru_32\")(features_in)\n",
    "H = layers.GRU(16, return_sequences=True, name=\"gru_16\")(H)\n",
    "H = layers.Dense(32, activation=\"relu\", name=\"dense_32\")(H)\n",
    "\n",
    "a_raw = layers.Dense(1, name=\"a_raw\")(H)\n",
    "a = layers.Lambda(lambda z: tf.cast(H_MAX, z.dtype) * tf.tanh(z),name=\"a\")(a_raw)\n",
    "\n",
    "policy = keras.Model(inputs=[features_in, dS_in, Z_T_in], outputs=a, name=\"policy_backbone\")\n",
    "policy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99fb6f-1035-4215-9e2d-99eaea705f81",
   "metadata": {},
   "source": [
    "## 3.7) Symbolic rollout (within the Keras graph)\n",
    "\n",
    "**What to do here (all tensor ops, no Python loops):**\n",
    "1. **Positions:** `a = policy(features)` → shape `(B, n_steps, 1)`\n",
    "2. **Turnover:** `a_prev` = shift of `a` with a zero at $(t=0)$;  \n",
    "   `turnover = |a - a_prev|`\n",
    "3. **Transaction cost:** get $(S_t)$ from features: `S_t = features[:,:,0] * S0` (then `[..., None]` to match dims);  \n",
    "   `cost = GAMMA * sum_t (S_t * turnover)` → shape `(B, 1)` then squeeze to `(B,)`\n",
    "4. **PnL:** `PnL = sum_t (a * dS)` → `(B,)`\n",
    "5. **Terminal wealth:** `X = -Z_T + PnL - cost` → `(B,)`\n",
    "\n",
    "**Monitoring tensors (for `add_metric`):**\n",
    "- `mean_X = mean(X)`\n",
    "- `mean_PnL = mean(PnL)`\n",
    "- `mean_cost = mean(cost)`\n",
    "- `turnover_mean = mean(turnover)` (averaged over batch & time)\n",
    "- `bound_frac = mean( I[|a| >= 0.95*H_MAX] )`\n",
    "- **Empirical tail stats on the batch** (for logging only):\n",
    "  - $(k = \\lceil (1-\\alpha)$,$\\text{batch\\_size} \\rceil)$\n",
    "  - `X_sorted = sort(X)` (ascending)\n",
    "  - `var_emp = X_sorted[k-1]`, `cvar_emp = mean(X_sorted[:k])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faac8bbb-92bb-4e25-a411-749346d0c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 7) Rollout\n",
    "# --------------------------\n",
    "\n",
    "a_prev = tf.concat([tf.zeros_like(a[:, :1, :]), a[:, :-1, :]], axis=1)\n",
    "turnover = tf.abs(a - a_prev)\n",
    "\n",
    "S_t = features_in[:, :, 0] * tf.cast(S0, tf.float32)\n",
    "S_t = tf.expand_dims(S_t, axis=-1)\n",
    "\n",
    "cost = GAMMA * tf.reduce_sum(S_t * turnover, axis=[1,2])\n",
    "PnL  = tf.reduce_sum(a * dS_in, axis=[1,2])\n",
    "\n",
    "X = -Z_T_in + PnL - cost  # shape: (batch,)\n",
    "\n",
    "mean_X = tf.reduce_mean(X)\n",
    "mean_PnL = tf.reduce_mean(PnL)\n",
    "mean_cost = tf.reduce_mean(cost)\n",
    "turnover_mu = tf.reduce_mean(turnover)\n",
    "bound_frac  = tf.reduce_mean(tf.cast(tf.abs(a) >= 0.95*H_MAX, tf.float32))\n",
    "\n",
    "k = tf.cast(tf.math.ceil((1.0 - ALPHA) * tf.cast(tf.shape(X)[0], tf.float32)), tf.int32)\n",
    "X_sorted = tf.sort(X)\n",
    "var_emp = X_sorted[k-1]\n",
    "cvar_emp = tf.reduce_mean(X_sorted[:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a61c0c-2892-49f2-9a2d-c74fb31e1ba1",
   "metadata": {},
   "source": [
    "## 3.8) RU CVaR head (loss layer)\n",
    "\n",
    "**What to do here:**\n",
    "- Add a custom layer **RUHead(α)** with a **trainable** scalar $(\\tau)$ (initialized e.g. near the baseline VaR or a constant):\n",
    "  $$\n",
    "  \\ell(X;\\tau) \\;=\\; \\frac{(\\tau - X)^+}{1-\\alpha} \\;-\\; \\tau\n",
    "  $$\n",
    "- Properties:\n",
    "  - Minimizing $( \\mathbb{E}[\\ell(X;\\tau)] )$ over $(\\tau)$ yields $(\\tau = \\text{VaR}_\\alpha(X))$.\n",
    "  - The minimized value equals $( \\text{CVaR}_\\alpha(X) )$.\n",
    "- Pipe the rollout wealth \\(X\\) into RUHead to get per-sample losses $(\\ell)$, then average.\n",
    "\n",
    "**Optional auxiliary penalty (keep small):**\n",
    "- $(\\beta \\cdot \\text{mean}(|X|))$ to nudge overall MAE down, but **do not** let it dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d308893-1efe-4f1c-8fd6-7125be322a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 8) CVaR head \n",
    "# --------------------------\n",
    "\n",
    "BETA = 0.01\n",
    "\n",
    "class RUHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, alpha,tau_init=-70.0,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = float(alpha)\n",
    "        self.tau_init = float(tau_init)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.tau = self.add_weight(\n",
    "            name=\"tau\",\n",
    "            shape=(),\n",
    "            initializer=tf.keras.initializers.Constant(self.tau_init),\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    \n",
    "    def call(self, X):\n",
    "        X = tf.cast(X, self.tau.dtype)\n",
    "\n",
    "        if X.shape.rank == 2 and X.shape[-1] == 1:\n",
    "            X = tf.squeeze(X, axis=-1)\n",
    "            \n",
    "        hinge = tf.nn.relu(self.tau - X)\n",
    "        ell = hinge / (1.0 - self.alpha) - self.tau\n",
    "        self.add_metric(tf.identity(self.tau), name=\"tau\", aggregation=\"mean\")\n",
    "        return ell\n",
    "\n",
    "\n",
    "def identify_loss(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "ru = RUHead(alpha=ALPHA, name=\"ru_head\")\n",
    "ell = ru(X)\n",
    "mae_penalty = BETA * tf.reduce_mean(tf.abs(X))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da69e5-f1a9-407f-b247-2efb12fecaa3",
   "metadata": {},
   "source": [
    "## 3.9) Compose the training model\n",
    "\n",
    "**What to do here:**\n",
    "- Create `loss_model = Model([features_in, dS_in, Z_T_in] → RUHead(X))`.\n",
    "- Attach `add_metric(...)` for the monitors from §6 (mean_X, mean_PnL, mean_cost, turnover_mean, bound_frac, var_emp, cvar_emp) and also expose `tau` from RUHead.\n",
    "- **Compile** with Adam (`learning_rate = LR`, optional `clipnorm=1.0`).\n",
    "- Use an **identity loss** (`return y_pred`) because the RUHead already outputs $(\\ell)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0066b22b-374d-4978-abc9-3cd3375148fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 9) Loss Model\n",
    "# --------------------------\n",
    "\n",
    "loss_model = keras.Model(inputs=[features_in, dS_in, Z_T_in],\n",
    "                         outputs=ell,\n",
    "                         name=\"loss\")\n",
    "\n",
    "loss_model.add_loss(mae_penalty)\n",
    "\n",
    "loss_model.add_metric(mean_X, name=\"mean_X\", aggregation=\"mean\")\n",
    "loss_model.add_metric(mean_PnL, name=\"mean_PnL\", aggregation=\"mean\")\n",
    "loss_model.add_metric(mean_cost, name=\"mean_Cost\", aggregation=\"mean\")\n",
    "loss_model.add_metric(turnover_mu, name=\"turnover_mean\", aggregation=\"mean\")\n",
    "loss_model.add_metric(bound_frac, name=\"bound_frac\", aggregation=\"mean\")\n",
    "loss_model.add_metric(cvar_emp, name=\"cvar_emp\", aggregation=\"mean\")\n",
    "loss_model.add_metric(var_emp, name=\"var_emp\", aggregation=\"mean\")\n",
    "\n",
    "mean_abs_X = tf.reduce_mean(tf.abs(X))\n",
    "loss_model.add_metric(mean_abs_X, name=\"mean_abs_X\", aggregation=\"mean\")\n",
    "\n",
    "loss_model.compile(optimizer=keras.optimizers.Adam(learning_rate = LR,\n",
    "                                                   clipnorm=1.0),\n",
    "                                                   loss=identify_loss,\n",
    "                                                   )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccce71-37d4-42e5-a260-f152576b3041",
   "metadata": {},
   "source": [
    "## 3.10) Training loop & callbacks\n",
    "\n",
    "**What to do here:**\n",
    "- Set callbacks:\n",
    "  - `ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5)`\n",
    "  - `EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)`\n",
    "  - `ModelCheckpoint(..., monitor=\"val_cvar_emp\", mode=\"min\")` to keep best tails\n",
    "  - `TerminateOnNaN()`\n",
    "- `fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=cbs)`\n",
    "- **How to read logs:**\n",
    "  - `loss / val_loss` ≈ batch mean of RU loss (proxy for CVaR)\n",
    "  - `tau` should settle near empirical VaR\n",
    "  - `cvar_emp` (val) should **decrease** vs baseline from §4\n",
    "  - `mean_cost`, `turnover_mean` give realism signals\n",
    "  - `bound_frac` near zero (not slamming into the clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1596643b-ca6b-4138-98ad-1852aa47dce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 95.9348 - tau: -69.9999 - mean_X: -23.0312 - mean_PnL: 0.2769 - mean_Cost: 0.2228 - turnover_mean: 0.0175 - bound_frac: 0.0000e+00 - cvar_emp: -95.9137 - var_emp: -70.1931 - mean_abs_X: 32.2693\n",
      "Epoch 1: val_cvar_emp improved from -inf to -92.98007, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 1: val_tau improved from -inf to -69.99994, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 1: val_mean_abs_X improved from inf to 31.54199, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 9s 437ms/step - loss: 95.9348 - tau: -69.9999 - mean_X: -23.0312 - mean_PnL: 0.2769 - mean_Cost: 0.2228 - turnover_mean: 0.0175 - bound_frac: 0.0000e+00 - cvar_emp: -95.9137 - var_emp: -70.1931 - mean_abs_X: 32.2693 - val_loss: 93.7831 - val_tau: -69.9999 - val_mean_X: -22.0010 - val_mean_PnL: 0.6942 - val_mean_Cost: 0.2212 - val_turnover_mean: 0.0173 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -92.9801 - val_var_emp: -67.0772 - val_mean_abs_X: 31.5420 - lr: 5.0000e-05\n",
      "Epoch 2/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 89.2871 - tau: -69.9998 - mean_X: -22.7805 - mean_PnL: 0.3482 - mean_Cost: 0.2174 - turnover_mean: 0.0171 - bound_frac: 0.0000e+00 - cvar_emp: -88.2968 - var_emp: -65.3167 - mean_abs_X: 30.4411\n",
      "Epoch 2: val_cvar_emp improved from -92.98007 to -86.15353, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 2: val_tau improved from -69.99994 to -69.99952, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 2: val_mean_abs_X improved from 31.54199 to 29.88120, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 6s 400ms/step - loss: 89.2871 - tau: -69.9998 - mean_X: -22.7805 - mean_PnL: 0.3482 - mean_Cost: 0.2174 - turnover_mean: 0.0171 - bound_frac: 0.0000e+00 - cvar_emp: -88.2968 - var_emp: -65.3167 - mean_abs_X: 30.4411 - val_loss: 87.5813 - val_tau: -69.9995 - val_mean_X: -22.0668 - val_mean_PnL: 0.6230 - val_mean_Cost: 0.2159 - val_turnover_mean: 0.0169 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -86.1535 - val_var_emp: -62.7138 - val_mean_abs_X: 29.8812 - lr: 5.0000e-05\n",
      "Epoch 3/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 84.0304 - tau: -69.9992 - mean_X: -22.8440 - mean_PnL: 0.5079 - mean_Cost: 0.2122 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -81.6489 - var_emp: -61.2567 - mean_abs_X: 28.9625\n",
      "Epoch 3: val_cvar_emp improved from -86.15353 to -80.11489, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 3: val_tau improved from -69.99952 to -69.99873, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 3: val_mean_abs_X improved from 29.88120 to 28.37176, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 472ms/step - loss: 84.0304 - tau: -69.9992 - mean_X: -22.8440 - mean_PnL: 0.5079 - mean_Cost: 0.2122 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -81.6489 - var_emp: -61.2567 - mean_abs_X: 28.9625 - val_loss: 82.7199 - val_tau: -69.9987 - val_mean_X: -22.1347 - val_mean_PnL: 0.5514 - val_mean_Cost: 0.2121 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -80.1149 - val_var_emp: -59.0913 - val_mean_abs_X: 28.3718 - lr: 5.0000e-05\n",
      "Epoch 4/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 80.0700 - tau: -69.9983 - mean_X: -22.8955 - mean_PnL: 0.2080 - mean_Cost: 0.2097 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -76.0296 - var_emp: -57.8899 - mean_abs_X: 27.6880\n",
      "Epoch 4: val_cvar_emp improved from -80.11489 to -74.51598, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 4: val_tau improved from -69.99873 to -69.99780, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 4: val_mean_abs_X improved from 28.37176 to 27.20064, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 496ms/step - loss: 80.0700 - tau: -69.9983 - mean_X: -22.8955 - mean_PnL: 0.2080 - mean_Cost: 0.2097 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -76.0296 - var_emp: -57.8899 - mean_abs_X: 27.6880 - val_loss: 78.9903 - val_tau: -69.9978 - val_mean_X: -22.1819 - val_mean_PnL: 0.5031 - val_mean_Cost: 0.2110 - val_turnover_mean: 0.0165 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -74.5160 - val_var_emp: -55.8869 - val_mean_abs_X: 27.2006 - lr: 5.0000e-05\n",
      "Epoch 5/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 77.0434 - tau: -69.9973 - mean_X: -22.6309 - mean_PnL: 0.4364 - mean_Cost: 0.2100 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -70.7224 - var_emp: -53.8246 - mean_abs_X: 26.6934\n",
      "Epoch 5: val_cvar_emp improved from -74.51598 to -69.27866, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 5: val_tau improved from -69.99780 to -69.99680, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 5: val_mean_abs_X improved from 27.20064 to 26.28979, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 482ms/step - loss: 77.0434 - tau: -69.9973 - mean_X: -22.6309 - mean_PnL: 0.4364 - mean_Cost: 0.2100 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -70.7224 - var_emp: -53.8246 - mean_abs_X: 26.6934 - val_loss: 76.2192 - val_tau: -69.9968 - val_mean_X: -22.2247 - val_mean_PnL: 0.4615 - val_mean_Cost: 0.2122 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -69.2787 - val_var_emp: -52.8293 - val_mean_abs_X: 26.2898 - lr: 5.0000e-05\n",
      "Epoch 6/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 74.8609 - tau: -69.9963 - mean_X: -22.7639 - mean_PnL: 0.5017 - mean_Cost: 0.2121 - turnover_mean: 0.0167 - bound_frac: 0.0000e+00 - cvar_emp: -66.4481 - var_emp: -51.0782 - mean_abs_X: 26.0070\n",
      "Epoch 6: val_cvar_emp improved from -69.27866 to -64.53131, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 6: val_tau improved from -69.99680 to -69.99574, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 6: val_mean_abs_X improved from 26.28979 to 25.50985, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 518ms/step - loss: 74.8609 - tau: -69.9963 - mean_X: -22.7639 - mean_PnL: 0.5017 - mean_Cost: 0.2121 - turnover_mean: 0.0167 - bound_frac: 0.0000e+00 - cvar_emp: -66.4481 - var_emp: -51.0782 - mean_abs_X: 26.0070 - val_loss: 74.4091 - val_tau: -69.9957 - val_mean_X: -22.2818 - val_mean_PnL: 0.4065 - val_mean_Cost: 0.2143 - val_turnover_mean: 0.0169 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -64.5313 - val_var_emp: -49.8060 - val_mean_abs_X: 25.5098 - lr: 5.0000e-05\n",
      "Epoch 7/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 73.6707 - tau: -69.9952 - mean_X: -22.9110 - mean_PnL: 0.4884 - mean_Cost: 0.2141 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -62.1585 - var_emp: -48.3664 - mean_abs_X: 25.4371\n",
      "Epoch 7: val_cvar_emp improved from -64.53131 to -60.35704, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 7: val_tau improved from -69.99574 to -69.99467, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 7: val_mean_abs_X improved from 25.50985 to 25.02465, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 517ms/step - loss: 73.6707 - tau: -69.9952 - mean_X: -22.9110 - mean_PnL: 0.4884 - mean_Cost: 0.2141 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -62.1585 - var_emp: -48.3664 - mean_abs_X: 25.4371 - val_loss: 73.3926 - val_tau: -69.9947 - val_mean_X: -22.3267 - val_mean_PnL: 0.3649 - val_mean_Cost: 0.2177 - val_turnover_mean: 0.0172 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -60.3570 - val_var_emp: -46.9329 - val_mean_abs_X: 25.0247 - lr: 5.0000e-05\n",
      "Epoch 8/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 73.0795 - tau: -69.9942 - mean_X: -22.7734 - mean_PnL: 0.4338 - mean_Cost: 0.2176 - turnover_mean: 0.0173 - bound_frac: 0.0000e+00 - cvar_emp: -58.3570 - var_emp: -45.7758 - mean_abs_X: 25.1106\n",
      "Epoch 8: val_cvar_emp improved from -60.35704 to -57.26875, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 8: val_tau improved from -69.99467 to -69.99362, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 8: val_mean_abs_X improved from 25.02465 to 24.80031, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 518ms/step - loss: 73.0795 - tau: -69.9942 - mean_X: -22.7734 - mean_PnL: 0.4338 - mean_Cost: 0.2176 - turnover_mean: 0.0173 - bound_frac: 0.0000e+00 - cvar_emp: -58.3570 - var_emp: -45.7758 - mean_abs_X: 25.1106 - val_loss: 72.9026 - val_tau: -69.9936 - val_mean_X: -22.3626 - val_mean_PnL: 0.3317 - val_mean_Cost: 0.2203 - val_turnover_mean: 0.0174 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -57.2688 - val_var_emp: -44.6840 - val_mean_abs_X: 24.8003 - lr: 5.0000e-05\n",
      "Epoch 9/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.7707 - tau: -69.9932 - mean_X: -22.6611 - mean_PnL: 0.4645 - mean_Cost: 0.2195 - turnover_mean: 0.0174 - bound_frac: 0.0000e+00 - cvar_emp: -55.6645 - var_emp: -43.7771 - mean_abs_X: 24.8214\n",
      "Epoch 9: val_cvar_emp improved from -57.26875 to -55.08150, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 9: val_tau improved from -69.99362 to -69.99267, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 9: val_mean_abs_X improved from 24.80031 to 24.66452, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 531ms/step - loss: 72.7707 - tau: -69.9932 - mean_X: -22.6611 - mean_PnL: 0.4645 - mean_Cost: 0.2195 - turnover_mean: 0.0174 - bound_frac: 0.0000e+00 - cvar_emp: -55.6645 - var_emp: -43.7771 - mean_abs_X: 24.8214 - val_loss: 72.5838 - val_tau: -69.9927 - val_mean_X: -22.3945 - val_mean_PnL: 0.3010 - val_mean_Cost: 0.2216 - val_turnover_mean: 0.0175 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -55.0815 - val_var_emp: -42.8143 - val_mean_abs_X: 24.6645 - lr: 5.0000e-05\n",
      "Epoch 10/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.5486 - tau: -69.9922 - mean_X: -22.8264 - mean_PnL: 0.3529 - mean_Cost: 0.2198 - turnover_mean: 0.0174 - bound_frac: 0.0000e+00 - cvar_emp: -54.2820 - var_emp: -42.2696 - mean_abs_X: 24.8213\n",
      "Epoch 10: val_cvar_emp improved from -55.08150 to -53.78451, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 10: val_tau improved from -69.99267 to -69.99170, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 10: val_mean_abs_X improved from 24.66452 to 24.46975, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 72.5486 - tau: -69.9922 - mean_X: -22.8264 - mean_PnL: 0.3529 - mean_Cost: 0.2198 - turnover_mean: 0.0174 - bound_frac: 0.0000e+00 - cvar_emp: -54.2820 - var_emp: -42.2696 - mean_abs_X: 24.8213 - val_loss: 72.4357 - val_tau: -69.9917 - val_mean_X: -22.4320 - val_mean_PnL: 0.2625 - val_mean_Cost: 0.2205 - val_turnover_mean: 0.0174 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -53.7845 - val_var_emp: -41.6727 - val_mean_abs_X: 24.4698 - lr: 5.0000e-05\n",
      "Epoch 11/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.4351 - tau: -69.9913 - mean_X: -22.6653 - mean_PnL: 0.5429 - mean_Cost: 0.2191 - turnover_mean: 0.0173 - bound_frac: 0.0000e+00 - cvar_emp: -53.2689 - var_emp: -41.1027 - mean_abs_X: 24.7022\n",
      "Epoch 11: val_cvar_emp improved from -53.78451 to -52.73569, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 11: val_tau improved from -69.99170 to -69.99078, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 11: val_mean_abs_X improved from 24.46975 to 24.42881, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 505ms/step - loss: 72.4351 - tau: -69.9913 - mean_X: -22.6653 - mean_PnL: 0.5429 - mean_Cost: 0.2191 - turnover_mean: 0.0173 - bound_frac: 0.0000e+00 - cvar_emp: -53.2689 - var_emp: -41.1027 - mean_abs_X: 24.7022 - val_loss: 72.3462 - val_tau: -69.9908 - val_mean_X: -22.4463 - val_mean_PnL: 0.2474 - val_mean_Cost: 0.2198 - val_turnover_mean: 0.0173 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -52.7357 - val_var_emp: -40.5945 - val_mean_abs_X: 24.4288 - lr: 5.0000e-05\n",
      "Epoch 12/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.3391 - tau: -69.9903 - mean_X: -22.7628 - mean_PnL: 0.5217 - mean_Cost: 0.2186 - turnover_mean: 0.0172 - bound_frac: 0.0000e+00 - cvar_emp: -52.5705 - var_emp: -40.1762 - mean_abs_X: 24.7011\n",
      "Epoch 12: val_cvar_emp improved from -52.73569 to -51.91267, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 12: val_tau improved from -69.99078 to -69.98989, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 12: val_mean_abs_X improved from 24.42881 to 24.42337, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 72.3391 - tau: -69.9903 - mean_X: -22.7628 - mean_PnL: 0.5217 - mean_Cost: 0.2186 - turnover_mean: 0.0172 - bound_frac: 0.0000e+00 - cvar_emp: -52.5705 - var_emp: -40.1762 - mean_abs_X: 24.7011 - val_loss: 72.2777 - val_tau: -69.9899 - val_mean_X: -22.4579 - val_mean_PnL: 0.2363 - val_mean_Cost: 0.2202 - val_turnover_mean: 0.0173 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -51.9127 - val_var_emp: -39.8975 - val_mean_abs_X: 24.4234 - lr: 5.0000e-05\n",
      "Epoch 13/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.2741 - tau: -69.9895 - mean_X: -22.6497 - mean_PnL: 0.3696 - mean_Cost: 0.2187 - turnover_mean: 0.0172 - bound_frac: 0.0000e+00 - cvar_emp: -51.6849 - var_emp: -39.6055 - mean_abs_X: 24.6399\n",
      "Epoch 13: val_cvar_emp improved from -51.91267 to -51.39265, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 13: val_tau improved from -69.98989 to -69.98904, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 13: val_mean_abs_X improved from 24.42337 to 24.41025, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 495ms/step - loss: 72.2741 - tau: -69.9895 - mean_X: -22.6497 - mean_PnL: 0.3696 - mean_Cost: 0.2187 - turnover_mean: 0.0172 - bound_frac: 0.0000e+00 - cvar_emp: -51.6849 - var_emp: -39.6055 - mean_abs_X: 24.6399 - val_loss: 72.2339 - val_tau: -69.9890 - val_mean_X: -22.4689 - val_mean_PnL: 0.2255 - val_mean_Cost: 0.2205 - val_turnover_mean: 0.0173 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -51.3926 - val_var_emp: -39.4098 - val_mean_abs_X: 24.4103 - lr: 5.0000e-05\n",
      "Epoch 14/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.2241 - tau: -69.9886 - mean_X: -22.7664 - mean_PnL: 0.6789 - mean_Cost: 0.2186 - turnover_mean: 0.0172 - bound_frac: 0.0000e+00 - cvar_emp: -51.5406 - var_emp: -39.1726 - mean_abs_X: 24.6837\n",
      "Epoch 14: val_cvar_emp improved from -51.39265 to -50.96574, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 14: val_tau improved from -69.98904 to -69.98818, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 14: val_mean_abs_X improved from 24.41025 to 24.38175, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 72.2241 - tau: -69.9886 - mean_X: -22.7664 - mean_PnL: 0.6789 - mean_Cost: 0.2186 - turnover_mean: 0.0172 - bound_frac: 0.0000e+00 - cvar_emp: -51.5406 - var_emp: -39.1726 - mean_abs_X: 24.6837 - val_loss: 72.1985 - val_tau: -69.9882 - val_mean_X: -22.4816 - val_mean_PnL: 0.2127 - val_mean_Cost: 0.2203 - val_turnover_mean: 0.0172 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -50.9657 - val_var_emp: -38.9593 - val_mean_abs_X: 24.3818 - lr: 5.0000e-05\n",
      "Epoch 15/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.1904 - tau: -69.9878 - mean_X: -22.5541 - mean_PnL: 0.4523 - mean_Cost: 0.2182 - turnover_mean: 0.0171 - bound_frac: 0.0000e+00 - cvar_emp: -51.2112 - var_emp: -38.8828 - mean_abs_X: 24.7784\n",
      "Epoch 15: val_cvar_emp improved from -50.96574 to -50.59523, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 15: val_tau improved from -69.98818 to -69.98733, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 15: val_mean_abs_X improved from 24.38175 to 24.31233, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 500ms/step - loss: 72.1904 - tau: -69.9878 - mean_X: -22.5541 - mean_PnL: 0.4523 - mean_Cost: 0.2182 - turnover_mean: 0.0171 - bound_frac: 0.0000e+00 - cvar_emp: -51.2112 - var_emp: -38.8828 - mean_abs_X: 24.7784 - val_loss: 72.1636 - val_tau: -69.9873 - val_mean_X: -22.4949 - val_mean_PnL: 0.1985 - val_mean_Cost: 0.2194 - val_turnover_mean: 0.0171 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -50.5952 - val_var_emp: -38.6910 - val_mean_abs_X: 24.3123 - lr: 5.0000e-05\n",
      "Epoch 16/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.1546 - tau: -69.9869 - mean_X: -22.7422 - mean_PnL: 0.4618 - mean_Cost: 0.2171 - turnover_mean: 0.0170 - bound_frac: 0.0000e+00 - cvar_emp: -50.8563 - var_emp: -38.4724 - mean_abs_X: 24.5984\n",
      "Epoch 16: val_cvar_emp improved from -50.59523 to -50.35886, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 16: val_tau improved from -69.98733 to -69.98647, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 16: val_mean_abs_X improved from 24.31233 to 24.29857, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 503ms/step - loss: 72.1546 - tau: -69.9869 - mean_X: -22.7422 - mean_PnL: 0.4618 - mean_Cost: 0.2171 - turnover_mean: 0.0170 - bound_frac: 0.0000e+00 - cvar_emp: -50.8563 - var_emp: -38.4724 - mean_abs_X: 24.5984 - val_loss: 72.1470 - val_tau: -69.9865 - val_mean_X: -22.4914 - val_mean_PnL: 0.2013 - val_mean_Cost: 0.2187 - val_turnover_mean: 0.0171 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -50.3589 - val_var_emp: -38.2722 - val_mean_abs_X: 24.2986 - lr: 5.0000e-05\n",
      "Epoch 17/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.1267 - tau: -69.9861 - mean_X: -22.7897 - mean_PnL: 0.2512 - mean_Cost: 0.2164 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -50.6285 - var_emp: -38.5998 - mean_abs_X: 24.5659\n",
      "Epoch 17: val_cvar_emp improved from -50.35886 to -50.16549, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 17: val_tau improved from -69.98647 to -69.98563, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 17: val_mean_abs_X improved from 24.29857 to 24.26890, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 509ms/step - loss: 72.1267 - tau: -69.9861 - mean_X: -22.7897 - mean_PnL: 0.2512 - mean_Cost: 0.2164 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -50.6285 - var_emp: -38.5998 - mean_abs_X: 24.5659 - val_loss: 72.1328 - val_tau: -69.9856 - val_mean_X: -22.4935 - val_mean_PnL: 0.1990 - val_mean_Cost: 0.2184 - val_turnover_mean: 0.0170 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -50.1655 - val_var_emp: -38.0863 - val_mean_abs_X: 24.2689 - lr: 5.0000e-05\n",
      "Epoch 18/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.1030 - tau: -69.9853 - mean_X: -22.7251 - mean_PnL: 0.3504 - mean_Cost: 0.2166 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -50.2847 - var_emp: -38.0420 - mean_abs_X: 24.4436\n",
      "Epoch 18: val_cvar_emp improved from -50.16549 to -49.91103, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 18: val_tau improved from -69.98563 to -69.98483, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 18: val_mean_abs_X improved from 24.26890 to 24.21325, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 507ms/step - loss: 72.1030 - tau: -69.9853 - mean_X: -22.7251 - mean_PnL: 0.3504 - mean_Cost: 0.2166 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -50.2847 - var_emp: -38.0420 - mean_abs_X: 24.4436 - val_loss: 72.1026 - val_tau: -69.9848 - val_mean_X: -22.5043 - val_mean_PnL: 0.1885 - val_mean_Cost: 0.2188 - val_turnover_mean: 0.0170 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.9110 - val_var_emp: -37.7255 - val_mean_abs_X: 24.2132 - lr: 5.0000e-05\n",
      "Epoch 19/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.0690 - tau: -69.9845 - mean_X: -22.7024 - mean_PnL: 0.3346 - mean_Cost: 0.2171 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -50.0587 - var_emp: -37.8204 - mean_abs_X: 24.3678\n",
      "Epoch 19: val_cvar_emp improved from -49.91103 to -49.74828, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 19: val_tau improved from -69.98483 to -69.98408, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 19: val_mean_abs_X improved from 24.21325 to 24.14574, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 515ms/step - loss: 72.0690 - tau: -69.9845 - mean_X: -22.7024 - mean_PnL: 0.3346 - mean_Cost: 0.2171 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -50.0587 - var_emp: -37.8204 - mean_abs_X: 24.3678 - val_loss: 72.0829 - val_tau: -69.9841 - val_mean_X: -22.5104 - val_mean_PnL: 0.1827 - val_mean_Cost: 0.2190 - val_turnover_mean: 0.0171 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.7483 - val_var_emp: -37.5980 - val_mean_abs_X: 24.1457 - lr: 5.0000e-05\n",
      "Epoch 20/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.0520 - tau: -69.9837 - mean_X: -22.6503 - mean_PnL: 0.4593 - mean_Cost: 0.2168 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -49.9041 - var_emp: -37.5785 - mean_abs_X: 24.3194\n",
      "Epoch 20: val_cvar_emp improved from -49.74828 to -49.64970, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 20: val_tau improved from -69.98408 to -69.98333, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 20: val_mean_abs_X improved from 24.14574 to 24.07318, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 510ms/step - loss: 72.0520 - tau: -69.9837 - mean_X: -22.6503 - mean_PnL: 0.4593 - mean_Cost: 0.2168 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -49.9041 - var_emp: -37.5785 - mean_abs_X: 24.3194 - val_loss: 72.0652 - val_tau: -69.9833 - val_mean_X: -22.5239 - val_mean_PnL: 0.1694 - val_mean_Cost: 0.2193 - val_turnover_mean: 0.0171 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.6497 - val_var_emp: -37.5130 - val_mean_abs_X: 24.0732 - lr: 5.0000e-05\n",
      "Epoch 21/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.0351 - tau: -69.9830 - mean_X: -22.6289 - mean_PnL: 0.5396 - mean_Cost: 0.2173 - turnover_mean: 0.0170 - bound_frac: 0.0000e+00 - cvar_emp: -50.0407 - var_emp: -37.5186 - mean_abs_X: 24.3550\n",
      "Epoch 21: val_cvar_emp improved from -49.64970 to -49.55864, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 21: val_tau improved from -69.98333 to -69.98258, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 21: val_mean_abs_X improved from 24.07318 to 24.04694, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 510ms/step - loss: 72.0351 - tau: -69.9830 - mean_X: -22.6289 - mean_PnL: 0.5396 - mean_Cost: 0.2173 - turnover_mean: 0.0170 - bound_frac: 0.0000e+00 - cvar_emp: -50.0407 - var_emp: -37.5186 - mean_abs_X: 24.3550 - val_loss: 72.0636 - val_tau: -69.9826 - val_mean_X: -22.5147 - val_mean_PnL: 0.1780 - val_mean_Cost: 0.2187 - val_turnover_mean: 0.0170 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.5586 - val_var_emp: -37.3589 - val_mean_abs_X: 24.0469 - lr: 5.0000e-05\n",
      "Epoch 22/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.0171 - tau: -69.9822 - mean_X: -22.6570 - mean_PnL: 0.5999 - mean_Cost: 0.2163 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -49.6107 - var_emp: -37.3374 - mean_abs_X: 24.2188\n",
      "Epoch 22: val_cvar_emp improved from -49.55864 to -49.50332, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 22: val_tau improved from -69.98258 to -69.98183, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 22: val_mean_abs_X improved from 24.04694 to 24.02741, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 503ms/step - loss: 72.0171 - tau: -69.9822 - mean_X: -22.6570 - mean_PnL: 0.5999 - mean_Cost: 0.2163 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -49.6107 - var_emp: -37.3374 - mean_abs_X: 24.2188 - val_loss: 72.0620 - val_tau: -69.9818 - val_mean_X: -22.5052 - val_mean_PnL: 0.1869 - val_mean_Cost: 0.2181 - val_turnover_mean: 0.0170 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.5033 - val_var_emp: -37.2392 - val_mean_abs_X: 24.0274 - lr: 5.0000e-05\n",
      "Epoch 23/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 72.0018 - tau: -69.9815 - mean_X: -22.6497 - mean_PnL: 0.5841 - mean_Cost: 0.2160 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -49.6931 - var_emp: -37.5028 - mean_abs_X: 24.1824\n",
      "Epoch 23: val_cvar_emp improved from -49.50332 to -49.47716, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 23: val_tau improved from -69.98183 to -69.98109, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 23: val_mean_abs_X improved from 24.02741 to 24.00150, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 72.0018 - tau: -69.9815 - mean_X: -22.6497 - mean_PnL: 0.5841 - mean_Cost: 0.2160 - turnover_mean: 0.0169 - bound_frac: 0.0000e+00 - cvar_emp: -49.6931 - var_emp: -37.5028 - mean_abs_X: 24.1824 - val_loss: 72.0568 - val_tau: -69.9811 - val_mean_X: -22.5008 - val_mean_PnL: 0.1909 - val_mean_Cost: 0.2177 - val_turnover_mean: 0.0169 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.4772 - val_var_emp: -37.1878 - val_mean_abs_X: 24.0015 - lr: 5.0000e-05\n",
      "Epoch 24/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9924 - tau: -69.9807 - mean_X: -22.7083 - mean_PnL: 0.5092 - mean_Cost: 0.2150 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.7482 - var_emp: -37.3464 - mean_abs_X: 24.1128\n",
      "Epoch 24: val_cvar_emp improved from -49.47716 to -49.39707, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 24: val_tau improved from -69.98109 to -69.98034, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 24: val_mean_abs_X improved from 24.00150 to 23.92125, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 506ms/step - loss: 71.9924 - tau: -69.9807 - mean_X: -22.7083 - mean_PnL: 0.5092 - mean_Cost: 0.2150 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.7482 - var_emp: -37.3464 - mean_abs_X: 24.1128 - val_loss: 72.0439 - val_tau: -69.9803 - val_mean_X: -22.5147 - val_mean_PnL: 0.1763 - val_mean_Cost: 0.2169 - val_turnover_mean: 0.0168 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.3971 - val_var_emp: -36.9272 - val_mean_abs_X: 23.9212 - lr: 5.0000e-05\n",
      "Epoch 25/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9750 - tau: -69.9800 - mean_X: -22.6930 - mean_PnL: 0.5529 - mean_Cost: 0.2152 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.5799 - var_emp: -37.4027 - mean_abs_X: 24.1645\n",
      "Epoch 25: val_cvar_emp did not improve from -49.39707\n",
      "\n",
      "Epoch 25: val_tau improved from -69.98034 to -69.97959, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 25: val_mean_abs_X did not improve from 23.92125\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 71.9750 - tau: -69.9800 - mean_X: -22.6930 - mean_PnL: 0.5529 - mean_Cost: 0.2152 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.5799 - var_emp: -37.4027 - mean_abs_X: 24.1645 - val_loss: 72.0492 - val_tau: -69.9796 - val_mean_X: -22.4926 - val_mean_PnL: 0.1994 - val_mean_Cost: 0.2180 - val_turnover_mean: 0.0169 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.4028 - val_var_emp: -37.1406 - val_mean_abs_X: 24.0021 - lr: 5.0000e-05\n",
      "Epoch 26/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9633 - tau: -69.9792 - mean_X: -22.7284 - mean_PnL: 0.5166 - mean_Cost: 0.2158 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.7463 - var_emp: -37.4213 - mean_abs_X: 24.2348\n",
      "Epoch 26: val_cvar_emp improved from -49.39707 to -49.35655, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 26: val_tau improved from -69.97959 to -69.97884, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 26: val_mean_abs_X did not improve from 23.92125\n",
      "14/14 [==============================] - 7s 516ms/step - loss: 71.9633 - tau: -69.9792 - mean_X: -22.7284 - mean_PnL: 0.5166 - mean_Cost: 0.2158 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.7463 - var_emp: -37.4213 - mean_abs_X: 24.2348 - val_loss: 72.0463 - val_tau: -69.9788 - val_mean_X: -22.5021 - val_mean_PnL: 0.1898 - val_mean_Cost: 0.2179 - val_turnover_mean: 0.0169 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.3565 - val_var_emp: -37.1270 - val_mean_abs_X: 23.9610 - lr: 5.0000e-05\n",
      "Epoch 27/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9507 - tau: -69.9785 - mean_X: -22.7268 - mean_PnL: 0.3484 - mean_Cost: 0.2152 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.6540 - var_emp: -37.2468 - mean_abs_X: 24.1833\n",
      "Epoch 27: val_cvar_emp improved from -49.35655 to -49.32510, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 27: val_tau improved from -69.97884 to -69.97810, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 27: val_mean_abs_X did not improve from 23.92125\n",
      "14/14 [==============================] - 7s 525ms/step - loss: 71.9507 - tau: -69.9785 - mean_X: -22.7268 - mean_PnL: 0.3484 - mean_Cost: 0.2152 - turnover_mean: 0.0168 - bound_frac: 0.0000e+00 - cvar_emp: -49.6540 - var_emp: -37.2468 - mean_abs_X: 24.1833 - val_loss: 72.0466 - val_tau: -69.9781 - val_mean_X: -22.4891 - val_mean_PnL: 0.2028 - val_mean_Cost: 0.2179 - val_turnover_mean: 0.0169 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.3251 - val_var_emp: -37.1230 - val_mean_abs_X: 23.9957 - lr: 5.0000e-05\n",
      "Epoch 28/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9426 - tau: -69.9778 - mean_X: -22.7635 - mean_PnL: 0.1424 - mean_Cost: 0.2148 - turnover_mean: 0.0167 - bound_frac: 0.0000e+00 - cvar_emp: -49.6646 - var_emp: -37.4935 - mean_abs_X: 24.2127\n",
      "Epoch 28: val_cvar_emp improved from -49.32510 to -49.29977, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 28: val_tau improved from -69.97810 to -69.97735, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 28: val_mean_abs_X did not improve from 23.92125\n",
      "14/14 [==============================] - 7s 527ms/step - loss: 71.9426 - tau: -69.9778 - mean_X: -22.7635 - mean_PnL: 0.1424 - mean_Cost: 0.2148 - turnover_mean: 0.0167 - bound_frac: 0.0000e+00 - cvar_emp: -49.6646 - var_emp: -37.4935 - mean_abs_X: 24.2127 - val_loss: 72.0492 - val_tau: -69.9773 - val_mean_X: -22.4868 - val_mean_PnL: 0.2036 - val_mean_Cost: 0.2164 - val_turnover_mean: 0.0168 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2998 - val_var_emp: -37.0897 - val_mean_abs_X: 23.9653 - lr: 5.0000e-05\n",
      "Epoch 29/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9299 - tau: -69.9770 - mean_X: -22.7615 - mean_PnL: 0.4284 - mean_Cost: 0.2135 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.7150 - var_emp: -37.3332 - mean_abs_X: 24.1381\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 29: val_cvar_emp improved from -49.29977 to -49.27128, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 29: val_tau improved from -69.97735 to -69.97660, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 29: val_mean_abs_X improved from 23.92125 to 23.90831, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 518ms/step - loss: 71.9299 - tau: -69.9770 - mean_X: -22.7615 - mean_PnL: 0.4284 - mean_Cost: 0.2135 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.7150 - var_emp: -37.3332 - mean_abs_X: 24.1381 - val_loss: 72.0459 - val_tau: -69.9766 - val_mean_X: -22.4916 - val_mean_PnL: 0.1977 - val_mean_Cost: 0.2153 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2713 - val_var_emp: -36.9412 - val_mean_abs_X: 23.9083 - lr: 5.0000e-05\n",
      "Epoch 30/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9192 - tau: -69.9764 - mean_X: -22.7236 - mean_PnL: 0.6630 - mean_Cost: 0.2130 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5148 - var_emp: -37.3852 - mean_abs_X: 24.1220\n",
      "Epoch 30: val_cvar_emp improved from -49.27128 to -49.25538, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 30: val_tau improved from -69.97660 to -69.97619, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 30: val_mean_abs_X did not improve from 23.90831\n",
      "14/14 [==============================] - 7s 520ms/step - loss: 71.9192 - tau: -69.9764 - mean_X: -22.7236 - mean_PnL: 0.6630 - mean_Cost: 0.2130 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5148 - var_emp: -37.3852 - mean_abs_X: 24.1220 - val_loss: 72.0405 - val_tau: -69.9762 - val_mean_X: -22.4903 - val_mean_PnL: 0.1993 - val_mean_Cost: 0.2156 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2554 - val_var_emp: -36.9430 - val_mean_abs_X: 23.9173 - lr: 2.5000e-05\n",
      "Epoch 31/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9149 - tau: -69.9760 - mean_X: -22.7046 - mean_PnL: 0.6009 - mean_Cost: 0.2133 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.2131 - var_emp: -37.1601 - mean_abs_X: 24.1020\n",
      "Epoch 31: val_cvar_emp improved from -49.25538 to -49.24084, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 31: val_tau improved from -69.97619 to -69.97578, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 31: val_mean_abs_X improved from 23.90831 to 23.90052, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 536ms/step - loss: 71.9149 - tau: -69.9760 - mean_X: -22.7046 - mean_PnL: 0.6009 - mean_Cost: 0.2133 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.2131 - var_emp: -37.1601 - mean_abs_X: 24.1020 - val_loss: 72.0361 - val_tau: -69.9758 - val_mean_X: -22.4966 - val_mean_PnL: 0.1927 - val_mean_Cost: 0.2152 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2408 - val_var_emp: -36.8384 - val_mean_abs_X: 23.9005 - lr: 2.5000e-05\n",
      "Epoch 32/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9138 - tau: -69.9756 - mean_X: -22.7097 - mean_PnL: 0.4438 - mean_Cost: 0.2126 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.3116 - var_emp: -37.3451 - mean_abs_X: 24.0145\n",
      "Epoch 32: val_cvar_emp improved from -49.24084 to -49.22837, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 32: val_tau improved from -69.97578 to -69.97537, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 32: val_mean_abs_X improved from 23.90052 to 23.84988, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 521ms/step - loss: 71.9138 - tau: -69.9756 - mean_X: -22.7097 - mean_PnL: 0.4438 - mean_Cost: 0.2126 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.3116 - var_emp: -37.3451 - mean_abs_X: 24.0145 - val_loss: 72.0296 - val_tau: -69.9754 - val_mean_X: -22.5057 - val_mean_PnL: 0.1827 - val_mean_Cost: 0.2144 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2284 - val_var_emp: -36.7334 - val_mean_abs_X: 23.8499 - lr: 2.5000e-05\n",
      "Epoch 33/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9109 - tau: -69.9752 - mean_X: -22.7143 - mean_PnL: 0.3953 - mean_Cost: 0.2121 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.3466 - var_emp: -37.0311 - mean_abs_X: 23.9800\n",
      "Epoch 33: val_cvar_emp improved from -49.22837 to -49.22221, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 33: val_tau improved from -69.97537 to -69.97500, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 33: val_mean_abs_X improved from 23.84988 to 23.82523, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 526ms/step - loss: 71.9109 - tau: -69.9752 - mean_X: -22.7143 - mean_PnL: 0.3953 - mean_Cost: 0.2121 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.3466 - var_emp: -37.0311 - mean_abs_X: 23.9800 - val_loss: 72.0242 - val_tau: -69.9750 - val_mean_X: -22.5122 - val_mean_PnL: 0.1762 - val_mean_Cost: 0.2144 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2222 - val_var_emp: -36.8296 - val_mean_abs_X: 23.8252 - lr: 2.5000e-05\n",
      "Epoch 34/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9077 - tau: -69.9748 - mean_X: -22.7705 - mean_PnL: 0.4581 - mean_Cost: 0.2120 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.4195 - var_emp: -37.2420 - mean_abs_X: 24.0277\n",
      "Epoch 34: val_cvar_emp improved from -49.22221 to -49.20181, saving model to results\\best_tail_by_cvar.weights_x.h5\n",
      "\n",
      "Epoch 34: val_tau improved from -69.97500 to -69.97466, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 34: val_mean_abs_X did not improve from 23.82523\n",
      "14/14 [==============================] - 7s 528ms/step - loss: 71.9077 - tau: -69.9748 - mean_X: -22.7705 - mean_PnL: 0.4581 - mean_Cost: 0.2120 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.4195 - var_emp: -37.2420 - mean_abs_X: 24.0277 - val_loss: 72.0281 - val_tau: -69.9747 - val_mean_X: -22.4992 - val_mean_PnL: 0.1894 - val_mean_Cost: 0.2147 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2018 - val_var_emp: -36.8599 - val_mean_abs_X: 23.8625 - lr: 2.5000e-05\n",
      "Epoch 35/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.9007 - tau: -69.9745 - mean_X: -22.7759 - mean_PnL: 0.3788 - mean_Cost: 0.2124 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.3887 - var_emp: -37.2293 - mean_abs_X: 24.0817\n",
      "Epoch 35: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 35: val_tau improved from -69.97466 to -69.97430, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 35: val_mean_abs_X did not improve from 23.82523\n",
      "14/14 [==============================] - 7s 540ms/step - loss: 71.9007 - tau: -69.9745 - mean_X: -22.7759 - mean_PnL: 0.3788 - mean_Cost: 0.2124 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.3887 - var_emp: -37.2293 - mean_abs_X: 24.0817 - val_loss: 72.0278 - val_tau: -69.9743 - val_mean_X: -22.4943 - val_mean_PnL: 0.1943 - val_mean_Cost: 0.2146 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2099 - val_var_emp: -36.8660 - val_mean_abs_X: 23.8647 - lr: 2.5000e-05\n",
      "Epoch 36/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8969 - tau: -69.9741 - mean_X: -22.7657 - mean_PnL: 0.2932 - mean_Cost: 0.2122 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5663 - var_emp: -37.0997 - mean_abs_X: 24.0486\n",
      "Epoch 36: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 36: val_tau improved from -69.97430 to -69.97391, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 36: val_mean_abs_X did not improve from 23.82523\n",
      "14/14 [==============================] - 7s 526ms/step - loss: 71.8969 - tau: -69.9741 - mean_X: -22.7657 - mean_PnL: 0.2932 - mean_Cost: 0.2122 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5663 - var_emp: -37.0997 - mean_abs_X: 24.0486 - val_loss: 72.0245 - val_tau: -69.9739 - val_mean_X: -22.5009 - val_mean_PnL: 0.1874 - val_mean_Cost: 0.2144 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2125 - val_var_emp: -36.7499 - val_mean_abs_X: 23.8345 - lr: 2.5000e-05\n",
      "Epoch 37/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8939 - tau: -69.9738 - mean_X: -22.6262 - mean_PnL: 0.6184 - mean_Cost: 0.2126 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.1558 - var_emp: -37.0747 - mean_abs_X: 23.9863\n",
      "Epoch 37: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 37: val_tau improved from -69.97391 to -69.97358, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 37: val_mean_abs_X did not improve from 23.82523\n",
      "14/14 [==============================] - 7s 523ms/step - loss: 71.8939 - tau: -69.9738 - mean_X: -22.6262 - mean_PnL: 0.6184 - mean_Cost: 0.2126 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.1558 - var_emp: -37.0747 - mean_abs_X: 23.9863 - val_loss: 72.0256 - val_tau: -69.9736 - val_mean_X: -22.4962 - val_mean_PnL: 0.1923 - val_mean_Cost: 0.2146 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2117 - val_var_emp: -36.9313 - val_mean_abs_X: 23.8482 - lr: 2.5000e-05\n",
      "Epoch 38/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8923 - tau: -69.9734 - mean_X: -22.7775 - mean_PnL: 0.4701 - mean_Cost: 0.2125 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.4921 - var_emp: -37.0258 - mean_abs_X: 24.0148\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 38: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 38: val_tau improved from -69.97358 to -69.97327, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 38: val_mean_abs_X improved from 23.82523 to 23.81663, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 506ms/step - loss: 71.8923 - tau: -69.9734 - mean_X: -22.7775 - mean_PnL: 0.4701 - mean_Cost: 0.2125 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.4921 - var_emp: -37.0258 - mean_abs_X: 24.0148 - val_loss: 72.0268 - val_tau: -69.9733 - val_mean_X: -22.5048 - val_mean_PnL: 0.1838 - val_mean_Cost: 0.2146 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2151 - val_var_emp: -36.9333 - val_mean_abs_X: 23.8166 - lr: 2.5000e-05\n",
      "Epoch 39/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8880 - tau: -69.9732 - mean_X: -22.7311 - mean_PnL: 0.3919 - mean_Cost: 0.2122 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.7236 - var_emp: -37.1915 - mean_abs_X: 24.0343\n",
      "Epoch 39: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 39: val_tau improved from -69.97327 to -69.97305, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 39: val_mean_abs_X did not improve from 23.81663\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 71.8880 - tau: -69.9732 - mean_X: -22.7311 - mean_PnL: 0.3919 - mean_Cost: 0.2122 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.7236 - var_emp: -37.1915 - mean_abs_X: 24.0343 - val_loss: 72.0294 - val_tau: -69.9731 - val_mean_X: -22.5042 - val_mean_PnL: 0.1844 - val_mean_Cost: 0.2147 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2193 - val_var_emp: -36.9418 - val_mean_abs_X: 23.8170 - lr: 1.2500e-05\n",
      "Epoch 40/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8882 - tau: -69.9730 - mean_X: -22.6236 - mean_PnL: 0.5262 - mean_Cost: 0.2128 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.3372 - var_emp: -37.1958 - mean_abs_X: 24.0044\n",
      "Epoch 40: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 40: val_tau improved from -69.97305 to -69.97284, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 40: val_mean_abs_X improved from 23.81663 to 23.79311, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 520ms/step - loss: 71.8882 - tau: -69.9730 - mean_X: -22.6236 - mean_PnL: 0.5262 - mean_Cost: 0.2128 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.3372 - var_emp: -37.1958 - mean_abs_X: 24.0044 - val_loss: 72.0287 - val_tau: -69.9728 - val_mean_X: -22.5089 - val_mean_PnL: 0.1796 - val_mean_Cost: 0.2145 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2209 - val_var_emp: -36.9109 - val_mean_abs_X: 23.7931 - lr: 1.2500e-05\n",
      "Epoch 41/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8850 - tau: -69.9727 - mean_X: -22.6873 - mean_PnL: 0.4840 - mean_Cost: 0.2124 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.4062 - var_emp: -37.0471 - mean_abs_X: 23.9728\n",
      "Epoch 41: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 41: val_tau improved from -69.97284 to -69.97263, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 41: val_mean_abs_X did not improve from 23.79311\n",
      "14/14 [==============================] - 7s 492ms/step - loss: 71.8850 - tau: -69.9727 - mean_X: -22.6873 - mean_PnL: 0.4840 - mean_Cost: 0.2124 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.4062 - var_emp: -37.0471 - mean_abs_X: 23.9728 - val_loss: 72.0308 - val_tau: -69.9726 - val_mean_X: -22.5037 - val_mean_PnL: 0.1848 - val_mean_Cost: 0.2145 - val_turnover_mean: 0.0167 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2163 - val_var_emp: -36.9516 - val_mean_abs_X: 23.8002 - lr: 1.2500e-05\n",
      "Epoch 42/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8826 - tau: -69.9725 - mean_X: -22.7860 - mean_PnL: 0.6103 - mean_Cost: 0.2121 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5224 - var_emp: -37.1918 - mean_abs_X: 24.0029\n",
      "Epoch 42: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 42: val_tau improved from -69.97263 to -69.97241, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 42: val_mean_abs_X did not improve from 23.79311\n",
      "14/14 [==============================] - 7s 504ms/step - loss: 71.8826 - tau: -69.9725 - mean_X: -22.7860 - mean_PnL: 0.6103 - mean_Cost: 0.2121 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5224 - var_emp: -37.1918 - mean_abs_X: 24.0029 - val_loss: 72.0343 - val_tau: -69.9724 - val_mean_X: -22.4995 - val_mean_PnL: 0.1889 - val_mean_Cost: 0.2144 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2252 - val_var_emp: -36.9210 - val_mean_abs_X: 23.8044 - lr: 1.2500e-05\n",
      "Epoch 43/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8800 - tau: -69.9723 - mean_X: -22.6475 - mean_PnL: 0.3954 - mean_Cost: 0.2125 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.4814 - var_emp: -37.1815 - mean_abs_X: 23.9213\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 43: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 43: val_tau improved from -69.97241 to -69.97220, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 43: val_mean_abs_X did not improve from 23.79311\n",
      "14/14 [==============================] - 7s 507ms/step - loss: 71.8800 - tau: -69.9723 - mean_X: -22.6475 - mean_PnL: 0.3954 - mean_Cost: 0.2125 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.4814 - var_emp: -37.1815 - mean_abs_X: 23.9213 - val_loss: 72.0352 - val_tau: -69.9722 - val_mean_X: -22.5002 - val_mean_PnL: 0.1881 - val_mean_Cost: 0.2143 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2277 - val_var_emp: -36.9202 - val_mean_abs_X: 23.7960 - lr: 1.2500e-05\n",
      "Epoch 44/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8782 - tau: -69.9721 - mean_X: -22.7520 - mean_PnL: 0.3743 - mean_Cost: 0.2126 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.3368 - var_emp: -37.1701 - mean_abs_X: 23.9510\n",
      "Epoch 44: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 44: val_tau improved from -69.97220 to -69.97210, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 44: val_mean_abs_X improved from 23.79311 to 23.78772, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 513ms/step - loss: 71.8782 - tau: -69.9721 - mean_X: -22.7520 - mean_PnL: 0.3743 - mean_Cost: 0.2126 - turnover_mean: 0.0166 - bound_frac: 0.0000e+00 - cvar_emp: -49.3368 - var_emp: -37.1701 - mean_abs_X: 23.9510 - val_loss: 72.0350 - val_tau: -69.9721 - val_mean_X: -22.5031 - val_mean_PnL: 0.1851 - val_mean_Cost: 0.2143 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2315 - val_var_emp: -36.9112 - val_mean_abs_X: 23.7877 - lr: 1.0000e-05\n",
      "Epoch 45/150\n",
      "14/14 [==============================] - ETA: 0s - loss: 71.8777 - tau: -69.9720 - mean_X: -22.7028 - mean_PnL: 0.4930 - mean_Cost: 0.2117 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5883 - var_emp: -37.0739 - mean_abs_X: 23.9478Restoring model weights from the end of the best epoch: 33.\n",
      "\n",
      "Epoch 45: val_cvar_emp did not improve from -49.20181\n",
      "\n",
      "Epoch 45: val_tau improved from -69.97210 to -69.97198, saving model to results\\best_tail_by_var.weights_x.h5\n",
      "\n",
      "Epoch 45: val_mean_abs_X improved from 23.78772 to 23.77912, saving model to results\\best_center_by_absX.weights_x.h5\n",
      "14/14 [==============================] - 7s 511ms/step - loss: 71.8777 - tau: -69.9720 - mean_X: -22.7028 - mean_PnL: 0.4930 - mean_Cost: 0.2117 - turnover_mean: 0.0165 - bound_frac: 0.0000e+00 - cvar_emp: -49.5883 - var_emp: -37.0739 - mean_abs_X: 23.9478 - val_loss: 72.0359 - val_tau: -69.9720 - val_mean_X: -22.5025 - val_mean_PnL: 0.1855 - val_mean_Cost: 0.2140 - val_turnover_mean: 0.0166 - val_bound_frac: 0.0000e+00 - val_cvar_emp: -49.2313 - val_var_emp: -36.8716 - val_mean_abs_X: 23.7791 - lr: 1.0000e-05\n",
      "Epoch 45: early stopping\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10) Training\n",
    "# --------------------------\n",
    "\n",
    "cbs = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", mode=\"min\",\n",
    "        patience=12, restore_best_weights=True, min_delta=1e-3, verbose=1\n",
    "    ),\n",
    "    # save weights for the best tail (CVaR)\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(RESULTS_DIR / \"best_tail_by_cvar.weights_x.h5\"),\n",
    "        monitor=\"val_cvar_emp\", mode=\"max\",\n",
    "        save_best_only=True, save_weights_only=True, verbose=1\n",
    "    ),\n",
    "    # optional: also save weights for best VaR (tau)\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(RESULTS_DIR / \"best_tail_by_var.weights_x.h5\"),\n",
    "        monitor=\"val_tau\", mode=\"max\",\n",
    "        save_best_only=True, save_weights_only=True, verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(RESULTS_DIR / \"best_center_by_absX.weights_x.h5\"),\n",
    "        monitor=\"val_mean_abs_X\", mode=\"min\",\n",
    "        save_best_only=True, save_weights_only=True, verbose=1\n",
    "    ),\n",
    "    keras.callbacks.TerminateOnNaN(),\n",
    "]\n",
    "\n",
    "history = loss_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd385b4-94be-47e2-9a35-db632be1ade2",
   "metadata": {},
   "source": [
    "## 3.11) Global validation snapshot\n",
    "\n",
    "**What to do here:**\n",
    "- Build a lightweight `eval_model` that outputs **[X, cost, turnover, PnL]** given inputs (reuses the trained backbone + rollout).\n",
    "- Run it on **all** validation paths (large batch; no shuffle).\n",
    "- Compute and print:\n",
    "  - `mean(X)` vs baseline mean of `-Z_T`\n",
    "  - `VaR_α(X)` and `CVaR_α(X)` vs baseline\n",
    "  - `mean(cost)` and **average total turnover per path** (sum over time, average over paths)\n",
    "- Optionally store arrays for Notebook 4 plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb6e791-93dc-4c5b-be45-8aba14fbaa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 150ms/step\n",
      "MAE: model 23.966 vs baseline 22.909\n",
      "=== Global Validation Evaluation ===\n",
      "Mean(XG): -22.66  vs baseline -22.909412384033203\n",
      "VaR_90 : -36.91  vs baseline -67.86\n",
      "CVaR_90: -49.85  vs baseline -105.29\n",
      "Mean Cost: 0.2140\n",
      "Avg Turnover: 4.1790\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11) Global Validation\n",
    "# --------------------------\n",
    "\n",
    "# Collect full validation set (inputs + targets)\n",
    "xb_full = {\"features\": [], \"dS\": [], \"Z_T\": []}\n",
    "yb_full = []\n",
    "\n",
    "for xb, yb in val_ds:   # iterate batches\n",
    "    for k in xb_full: \n",
    "        xb_full[k].append(xb[k].numpy())\n",
    "    yb_full.append(yb.numpy())\n",
    "\n",
    "# Concatenate into single arrays\n",
    "for k in xb_full:\n",
    "    xb_full[k] = np.concatenate(xb_full[k], axis=0)\n",
    "yb_full = np.concatenate(yb_full, axis=0)\n",
    "\n",
    "\n",
    "eval_model = keras.Model(\n",
    "    inputs=[features_in, dS_in, Z_T_in],\n",
    "    outputs=[X, cost, turnover, PnL]\n",
    ")\n",
    "\n",
    "X_val, cost_val, turnover_val, PnL_val = eval_model.predict(\n",
    "    [xb_full[\"features\"], xb_full[\"dS\"], xb_full[\"Z_T\"]],\n",
    "    batch_size=1024, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "N = X_val.shape[0]\n",
    "\n",
    "# Mean\n",
    "mean_XG = np.mean(X_val)\n",
    "\n",
    "# VaR at 90% (10th percentile)\n",
    "k = int(np.ceil((1 - ALPHA) * N))\n",
    "X_sorted = np.sort(X_val)\n",
    "VaR_90 = X_sorted[k-1]\n",
    "\n",
    "# CVaR at 90% (average of worst 10%)\n",
    "CVaR_90 = np.mean(X_sorted[:k])\n",
    "\n",
    "# Mean cost\n",
    "mean_cost = np.mean(cost_val)\n",
    "\n",
    "# Total turnover per path (sum across timesteps, then average over paths)\n",
    "turnover_total = np.mean(np.sum(turnover_val, axis=1))\n",
    "\n",
    "X_zero = -xb_full[\"Z_T\"]  # wealth is just -Z_T\n",
    "\n",
    "# Zero-hedge baseline\n",
    "X0_sorted = np.sort(X_zero)\n",
    "mean_baseline = np.mean(X_zero)\n",
    "\n",
    "\n",
    "var_baseline = X0_sorted[k-1]\n",
    "\n",
    "hedged_mae = np.mean(np.abs(X_val))             # from eval_model outputs\n",
    "baseline_mae = np.mean(np.abs(-xb_full[\"Z_T\"])) # zero-hedge\n",
    "print(f\"MAE: model {hedged_mae:.3f} vs baseline {baseline_mae:.3f}\")\n",
    "\n",
    "print(\"=== Global Validation Evaluation ===\")\n",
    "print(f\"Mean(XG): {mean_XG:.2f}  vs baseline {mean_baseline}\")\n",
    "print(f\"VaR_90 : {VaR_90:.2f}  vs baseline {var_baseline:.2f}\")\n",
    "print(f\"CVaR_90: {CVaR_90:.2f}  vs baseline {val_cvar_baseline:.2f}\")\n",
    "print(f\"Mean Cost: {mean_cost:.4f}\")\n",
    "print(f\"Avg Turnover: {turnover_total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f48cd4-4ae6-4bcb-a636-13c6a0b55f61",
   "metadata": {},
   "source": [
    "## 3.12) Test set evaluation & saving artifacts\n",
    "\n",
    "**What to do here:**\n",
    "- Repeat §11 on `test_ds` and report the same metrics.\n",
    "- **Save** to `results/hedging_eval_test_keras.npz`, e.g. keys:\n",
    "  - `X` (terminal wealth), `Z_T`, `PnL`, `cost`, `turnover`, and possibly positions `a`\n",
    "- These feed Notebook 4 for richer diagnostics/plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7354ad19-6eb4-41ee-9dce-35f3268727ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 154ms/step\n",
      "=== Global Test Evaluation ===\n",
      "Mean(X): -23.06  vs baseline -22.91\n",
      "VaR_90 : -36.73  vs baseline -67.86\n",
      "CVaR_90: -49.67  vs baseline -105.29\n",
      "Mean Cost: 0.2110\n",
      "Avg Turnover: 4.1519\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 12) Testing\n",
    "# --------------------------\n",
    "\n",
    "xb_full = {\"features\": [], \"dS\": [], \"Z_T\": []}\n",
    "yb_full = []\n",
    "\n",
    "for xb, yb in test_ds:   # iterate batches\n",
    "    for k in xb_full: \n",
    "        xb_full[k].append(xb[k].numpy())\n",
    "    yb_full.append(yb.numpy())\n",
    "\n",
    "for k in xb_full:\n",
    "    xb_full[k] = np.concatenate(xb_full[k], axis=0)\n",
    "yb_full = np.concatenate(yb_full, axis=0)\n",
    "\n",
    "X_test, cost_test, turnover_test, PnL_test = eval_model.predict(\n",
    "    [xb_full[\"features\"], xb_full[\"dS\"], xb_full[\"Z_T\"]],\n",
    "    batch_size=1024,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "N = X_test.shape[0]\n",
    "\n",
    "mean_XG = np.mean(X_test)\n",
    "\n",
    "k = int(np.ceil((1 - ALPHA) * N))\n",
    "X_sorted = np.sort(X_test)\n",
    "VaR_90 = X_sorted[k-1]\n",
    "CVaR_90 = np.mean(X_sorted[:k])\n",
    "\n",
    "mean_cost = np.mean(cost_test)\n",
    "turnover_total = np.mean(np.sum(turnover_test, axis=1))\n",
    "\n",
    "print(\"=== Global Test Evaluation ===\")\n",
    "print(f\"Mean(X): {mean_XG:.2f}  vs baseline {mean_baseline:.2f}\")\n",
    "print(f\"VaR_90 : {VaR_90:.2f}  vs baseline {var_baseline:.2f}\")\n",
    "print(f\"CVaR_90: {CVaR_90:.2f}  vs baseline {val_cvar_baseline:.2f}\")\n",
    "print(f\"Mean Cost: {mean_cost:.4f}\")\n",
    "print(f\"Avg Turnover: {turnover_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90217f1a-7167-4833-8dbd-e55373f3381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: results\\hedging_eval_test_keras_xi.npz  (V_T (5000,), Z_T (5000,))\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 13) SAVE\n",
    "# --------------------------\n",
    "# Save V_T (hedged terminal portfolio) and Z_T (payoff) to NPZ\n",
    "#   X_test, cost_test, turnover_test, PnL_test = eval_model.predict(...)\n",
    "#   xb_full[\"Z_T\"] from building the test set\n",
    "\n",
    "V_T_test = (PnL_test.reshape(-1) - cost_test.reshape(-1)).astype(np.float32)  # (N,)\n",
    "Z_T_test = xb_full[\"Z_T\"].reshape(-1).astype(np.float32)                      # (N,)\n",
    "\n",
    "# Sanity check\n",
    "assert V_T_test.shape == Z_T_test.shape, f\"Shape mismatch: {V_T_test.shape} vs {Z_T_test.shape}\"\n",
    "\n",
    "out_path = RESULTS_DIR / \"hedging_eval_test_keras_xi.npz\"\n",
    "np.savez_compressed(out_path, V_T=V_T_test, Z_T=Z_T_test)\n",
    "print(f\"Saved: {out_path}  (V_T {V_T_test.shape}, Z_T {Z_T_test.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed381c62-c431-4be9-85d3-12c93abf0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_model.load_weights(RESULTS_DIR / \"best_tail_by_cvar.weights_x.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd04a9-1747-4bc4-919f-17ae72adb8b5",
   "metadata": {},
   "source": [
    "## 3.13) Interpreting results\n",
    "\n",
    "**What good looks like (directionally):**\n",
    "- `CVaR_α(X)` **much higher** (less negative) than baseline \\( \\text{CVaR}_\\alpha(-Z_T) \\)  \n",
    "  (remember: more negative = worse loss; moving toward zero is improvement)\n",
    "- `VaR_α(X)` improved similarly\n",
    "- `mean(X)` may stay negative (you’re minimizing tail, not MSE); small positive is a bonus\n",
    "- `mean_cost` and `turnover` within realistic bounds (gives credibility)\n",
    "- `bound_frac` small (not exploiting the clamp edge)\n",
    "\n",
    "**If it looks too good to be true:**\n",
    "- Check no look-ahead in features (§2)\n",
    "- Ensure `dS` aligns with time \\(t\\) features\n",
    "- Confirm `Z_T` shape is 1D and used only in `X = -Z_T + ...`\n",
    "\n",
    "---\n",
    "\n",
    "## 3.14) Hyperparameter guide (what changes what)\n",
    "\n",
    "- **`ALPHA` (tail level):**  \n",
    "  Higher (e.g., 0.95) ⇒ focuses on *extreme* worst tail, typically stronger tail protection, possibly larger mean error.  \n",
    "  Lower (e.g., 0.80) ⇒ softer tail, may improve average metrics but weaker extreme-loss control.\n",
    "- **`GAMMA` (cost):**  \n",
    "  Higher ⇒ discourages turnover, reduces cost & noise, might leave some risk unhedged.  \n",
    "  Lower ⇒ more active hedging, better fit, but higher costs / overfitting risk.\n",
    "- **`H_MAX` (clamp):**  \n",
    "  Lower ⇒ policies stay small; safer but under-powered.  \n",
    "  Higher ⇒ more capacity; watch `bound_frac`.\n",
    "- **Backbone size (GRU widths):**  \n",
    "  Larger ⇒ more expressive; use dropout or stronger in case of overfitting.\n",
    "- **`LR`, `BATCH`:**  \n",
    "  Tail objectives benefit from moderately large batches (more stable tail estimate). Too small batches produce noisy VaR/CVaR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clustering-env)",
   "language": "python",
   "name": "clustering-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
