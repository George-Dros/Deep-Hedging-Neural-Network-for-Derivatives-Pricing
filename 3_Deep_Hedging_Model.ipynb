{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d5a508-c46d-4e57-899a-bbdf36efc517",
   "metadata": {},
   "source": [
    "# 3. Deep Hedging Model\n",
    "\n",
    "In this notebook, we implement and train a neural network that learns to hedge a European call option dynamically using simulated market data.\n",
    "\n",
    "We treat this as a reinforcement learning-style problem, where the model (policy) observes the market, makes hedging decisions at each time step, and aims to replicate the option payoff as closely as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ 3.1 Neural Network Architecture Setup\n",
    "\n",
    "We begin by defining the architecture of the hedging model.\n",
    "\n",
    "### Objective:\n",
    "\n",
    "The neural network takes as input a **market state vector** at each time step and outputs a **hedging decision** â€” specifically, the amount of the underlying asset to hold (hedge ratio).\n",
    "\n",
    "This is a function:\n",
    "\n",
    "$$\n",
    "a_t = \\pi_\\theta(s_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $( s_t \\in \\mathbb{R}^4 )$ is the state at time $( t )$ (from our feature vector)\n",
    "- $( \\pi_\\theta )$ is the policy network (a neural net with parameters $( \\theta )$)\n",
    "- $( a_t )$ is the action: the number of units of the underlying asset to hold\n",
    "\n",
    "---\n",
    "\n",
    "### Input Features\n",
    "\n",
    "Each state $( s_t )$ includes:\n",
    "\n",
    "1. **Normalized Price**: $( S_t / S_0 )$\n",
    "2. **Time to Maturity**: $( (T - t)/T )$\n",
    "3. **Log Return**: $( \\log(S_t / S_{t-1}) )$\n",
    "4. **Simulated Volatility**: $( \\sigma_t )$\n",
    "\n",
    "These are taken from the training dataset constructed in Notebook 2.\n",
    "\n",
    "---\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "We use a simple feedforward (fully connected) neural network applied independently at each time step:\n",
    "\n",
    "- **Input layer**: 4 features\n",
    "- **Hidden layers**: 2â€“3 layers with ReLU activation\n",
    "- **Output layer**: 1 value per time step (hedge ratio)\n",
    "- **Output activation**: Linear (no constraint on hedge ratio)\n",
    "\n",
    "This network learns a function from observed market conditions to trading decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### Time Step Independence\n",
    "\n",
    "Note: We apply the network **separately at each time step**, treating each $( s_t )$ as independent.  \n",
    "This is sufficient for vanilla deep hedging and simplifies training compared to using an RNN or transformer.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "We now define the neural network and prepare it to learn a mapping:\n",
    "\n",
    "$$\n",
    "s_t \\mapsto a_t \\quad \\text{for each time step } t \\in \\{0, \\ldots, T-1\\}\n",
    "$$\n",
    "\n",
    "In the next section, we simulate the portfolio evolution from these decisions and define the training objective based on hedging performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff06fac-ac7d-4287-b1bc-d308954f9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, torch, platform, os, site, json, textwrap, subprocess, pathlib, gc\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import trange\n",
    "# import time\n",
    "# from IPython.display import clear_output\n",
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c929c3cf-c08b-4266-b785-b4516eae2a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache();  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093382c5-8f8a-40a1-9e15-af8a2a776ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"data/deep_hedging_data.npz\")\n",
    "X_train = data[\"X_train\"]            # (8000, T, 4)\n",
    "Y_train = data[\"Y_train\"]            # (8000,)\n",
    "X_test  = data[\"X_test\"]             # (2000, T, 4)\n",
    "Y_test  = data[\"Y_test\"]             # (2000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee57b39-bcce-48c0-b7fd-97b6bff44c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%  (300/300)\n",
      "\n",
      "Saved results to results/hedging_eval_test.npz  (shape: torch.Size([2000]))  in 0.2s\n"
     ]
    }
   ],
   "source": [
    "import torch, math, time, os, numpy as np\n",
    "from torch import nn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0.  Load tensors (already have X_train, Y_train, X_test, Y_test in RAM)\n",
    "#     If not, load them here with np.load(...)\n",
    "# ------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32, device=device).squeeze(-1)\n",
    "X_test  = torch.tensor(X_test , dtype=torch.float32, device=device)\n",
    "Y_test  = torch.tensor(Y_test , dtype=torch.float32, device=device).squeeze(-1)\n",
    "\n",
    "N_train, T, _ = X_train.shape        # 8000 Ã— T Ã— 4\n",
    "N_test  = X_test.shape[0]\n",
    "\n",
    "# quick imbalance print\n",
    "zero_pct = (Y_train == 0).float().mean().item() * 100\n",
    "print(f\"{zero_pct:.1f}% of training pay-offs are exactly zero\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build spot matrices from feature-0 (normalised price)\n",
    "# ------------------------------------------------------------\n",
    "S0   = 100.0                         # same S0 you used in Notebook 2\n",
    "S_t  = X_train[:, :, 0] * S0         # [8000, T]  â€” already on GPU\n",
    "S_te = X_test  [:, :, 0] * S0        # [2000, T]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  GRU hedge network\n",
    "# ------------------------------------------------------------\n",
    "class HedgingGRU(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):            # x: [N, T, 4]\n",
    "        h, _ = self.gru(x)\n",
    "        return self.out(h).squeeze(-1)   # [N, T]\n",
    "\n",
    "model = HedgingGRU().to(device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Optimiser & scheduler\n",
    "# ------------------------------------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "              optimizer, factor=0.5, patience=20, verbose=False)\n",
    "\n",
    "n_epochs = 300\n",
    "tc       = 0.0\n",
    "alpha    = 0.9\n",
    "logfile  = open(\"training_log.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "def progress_bar(i, total, bar_len=40):\n",
    "    pct  = i / total\n",
    "    fill = \"â–ˆ\" * int(bar_len * pct)\n",
    "    bar  = f\"[{fill:<{bar_len}}] {pct:6.1%}  ({i}/{total})\"\n",
    "    clear_output(wait=True); print(bar)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Training loop\n",
    "# ------------------------------------------------------------\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # hedge ratios\n",
    "    H   = model(X_train)                                # [N, T]\n",
    "    dH  = torch.diff(H, 1, prepend=torch.zeros_like(H[:, :1]))\n",
    "\n",
    "    trade_vol = dH.abs() * S_t\n",
    "    cash      = (-dH * S_t - tc * trade_vol).sum(1)\n",
    "    V_T       = cash + H[:, -1] * S_t[:, -1]            # [N]\n",
    "\n",
    "        # --- ITM-CVaR(Î±)  +  Î²Â·MSE (to penalise overshoot) ----------------\n",
    "    itm   = Y_train > 0\n",
    "    itm_err = (V_T - Y_train)[itm]        # P&L error on ITM paths\n",
    "\n",
    "    beta = 0.10                           # 0.01â€“0.10 works; start with 0.05\n",
    "\n",
    "    if itm_err.numel():\n",
    "        # 1) CVaR part (same as before)\n",
    "        short = torch.clamp(itm_err, max=0)               # only short-falls\n",
    "        k     = max(int((1 - alpha) * itm_err.numel()), 1)\n",
    "        VaR, _ = torch.kthvalue(short, k)\n",
    "        cvar   = short[short <= VaR].mean()               # â‰¤ 0\n",
    "        # 2) symmetric MSE part (penalise overshoot too)\n",
    "        mse_itm = (itm_err**2).mean()\n",
    "        # 3) combined objective\n",
    "        loss = -cvar + beta * mse_itm                     # minimise both\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        msg = (f\"epoch {epoch:4d}  -CVaR {(-loss).item():10.4f}  \"\n",
    "               f\"lr {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "        logfile.write(msg + \"\\n\"); logfile.flush()\n",
    "        print(msg)\n",
    "    progress_bar(epoch + 1, n_epochs)\n",
    "\n",
    "logfile.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  TEST EVALUATION & SAVE\n",
    "# ------------------------------------------------------------\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    H_te   = model(X_test)                               # [2000, T]\n",
    "    dH_te  = torch.diff(H_te, 1, prepend=torch.zeros_like(H_te[:, :1]))\n",
    "    trade_vol = dH_te.abs() * S_te\n",
    "    cash   = (-dH_te * S_te - tc * trade_vol).sum(1)\n",
    "    V_T_te = cash + H_te[:, -1] * S_te[:, -1]            # [2000]\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    \"results/hedging_eval_test.npz\",\n",
    "    V_T = V_T_te.cpu().numpy().astype(np.float32),\n",
    "    Z_T = Y_test.cpu().numpy().astype(np.float32)\n",
    ")\n",
    "print(f\"\\nSaved results to results/hedging_eval_test.npz  \"\n",
    "      f\"(shape: {V_T_te.shape})  in {time.time()-start:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784cecb0-bfdf-4ea0-bc7a-0da781c35ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“  Saved weights to  models/gru_beta010_final.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/gru_beta010_final.pt\")\n",
    "print(\"âœ“  Saved weights to  models/gru_beta010_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clustering-env)",
   "language": "python",
   "name": "clustering-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
